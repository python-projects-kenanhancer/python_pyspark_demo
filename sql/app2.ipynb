{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T22:21:39.536903Z",
     "start_time": "2023-11-11T22:21:38.105338Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 22:21:39 WARN Utils: Your hostname, kenans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.102 instead (on interface en0)\n",
      "23/11/11 22:21:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Exception in thread \"main\" java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.unsafe.array.ByteArrayMethods.<clinit>(ByteArrayMethods.java:54)\n",
      "\tat org.apache.spark.internal.config.package$.<init>(package.scala:1006)\n",
      "\tat org.apache.spark.internal.config.package$.<clinit>(package.scala)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.$anonfun$loadEnvironmentArguments$3(SparkSubmitArguments.scala:157)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.loadEnvironmentArguments(SparkSubmitArguments.scala:157)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:115)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:990)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:990)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make private java.nio.DirectByteBuffer(long,int) accessible: module java.base does not \"opens java.nio\" to unnamed module @654b5005\n",
      "\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:357)\n",
      "\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\n",
      "\tat java.base/java.lang.reflect.Constructor.checkCanSetAccessible(Constructor.java:188)\n",
      "\tat java.base/java.lang.reflect.Constructor.setAccessible(Constructor.java:181)\n",
      "\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:56)\n",
      "\t... 13 more\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from pyspark.sql import functions as sf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:186\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/context.py:378\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/context.py:133\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    136\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/context.py:327\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 327\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/java_gateway.py:105\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    108\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# from pyspark.sql import functions as sf\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sql2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88445d221491b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:03.243709Z",
     "start_time": "2023-11-08T09:09:54.132122Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|count|\n",
      "+------+-----+\n",
      "| Alice|    3|\n",
      "|   Bob|    3|\n",
      "| james|    2|\n",
      "| Cathy|    2|\n",
      "|andrew|    1|\n",
      "|   ann|    1|\n",
      "+------+-----+\n"
     ]
    }
   ],
   "source": [
    "peopleV2 = spark.createDataFrame([Row(name=\"andrew\", age=32),\n",
    "                                  Row(name=\"james\", age=23),\n",
    "                                  Row(name=\"james\", age=4),\n",
    "                                  Row(name=\"ann\", age=12),\n",
    "                                  Row(name=\"Alice\", age=2),\n",
    "                                  Row(name=\"Alice\", age=3),\n",
    "                                  Row(name=\"Alice\", age=6),\n",
    "                                  Row(name=\"Bob\", age=5),\n",
    "                                  Row(name=\"Bob\", age=10),\n",
    "                                  Row(name=\"Bob\", age=12),\n",
    "                                  Row(name=\"Cathy\", age=7),\n",
    "                                  Row(name=\"Cathy\", age=8)])\n",
    "\n",
    "peopleV2.groupby('name') \\\n",
    "    .agg(count('*').alias('count')) \\\n",
    "    .sort(col('count').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35770392b038649e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:05.242004Z",
     "start_time": "2023-11-08T09:10:03.243597Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|count|\n",
      "+------+-----+\n",
      "|andrew|    1|\n",
      "|   ann|    1|\n",
      "| james|    2|\n",
      "| Cathy|    2|\n",
      "| Alice|    3|\n",
      "|   Bob|    3|\n",
      "+------+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "peopleV2.groupby('name') \\\n",
    "    .agg({\"*\": 'count'}) \\\n",
    "    .withColumnRenamed('count(1)', 'count') \\\n",
    "    .sort('count', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23dd7aae826efd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:06.493797Z",
     "start_time": "2023-11-08T09:10:05.242127Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|count|\n",
      "+------+-----+\n",
      "|andrew|    1|\n",
      "|   ann|    1|\n",
      "| james|    2|\n",
      "| Cathy|    2|\n",
      "| Alice|    3|\n",
      "|   Bob|    3|\n",
      "+------+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "peopleV2.groupby('name') \\\n",
    "    .agg(count('*').alias('count')) \\\n",
    "    .sort(col('count').asc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c0c91df65765dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:07.658860Z",
     "start_time": "2023-11-08T09:10:06.494321Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n",
      "|  name|min_age|max_age|\n",
      "+------+-------+-------+\n",
      "| Alice|      2|      6|\n",
      "| james|      4|     23|\n",
      "|   Bob|      5|     12|\n",
      "| Cathy|      7|      8|\n",
      "|   ann|     12|     12|\n",
      "|andrew|     32|     32|\n",
      "+------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "peopleV2.groupby('name') \\\n",
    "    .agg(min('age').alias('min_age'), max('age').alias('max_age')) \\\n",
    "    .sort(col('min_age').asc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417f18eac0dce4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:12:57.350814Z",
     "start_time": "2023-11-08T09:12:56.630068Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|  name|min_age|\n",
      "+------+-------+\n",
      "| Alice|      2|\n",
      "|   Bob|      5|\n",
      "| Cathy|      7|\n",
      "|andrew|     32|\n",
      "|   ann|     12|\n",
      "| james|      4|\n",
      "+------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenanhancer/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/kenanhancer/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/kenanhancer/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/kenanhancer/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/kenanhancer/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/kenanhancer/Documents/projects/python-projects-kenanhancer/python_pyspark_demo/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('int', PandasUDFType.GROUPED_AGG)\n",
    "def min_udf(v):\n",
    "    return v.min()\n",
    "\n",
    "\n",
    "peopleV2.groupby('name').agg(min_udf(peopleV2.age).alias('min_age')).sort('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9bcdc95135455d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:10.214517Z",
     "start_time": "2023-11-08T09:10:10.212972Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
