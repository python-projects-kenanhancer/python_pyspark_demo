{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c4056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Welcome to Comprehensive Pandas Exercises!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Pandas Coding Exercises: From Basic to Advanced\n",
    "# Using Various Python Data Types for Complete Understanding\n",
    "\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"🚀 Welcome to Comprehensive Pandas Exercises!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf0ec2",
   "metadata": {},
   "source": [
    "# 📚 LEVEL 1: BASIC PANDAS OPERATIONS\n",
    "\n",
    "## Exercise 1.1: Creating DataFrames from Different Data Types\n",
    "\n",
    "Let's start with the fundamentals - creating DataFrames from various Python data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01732a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Creating DataFrame from LIST of dictionaries:\n",
      "--------------------------------------------------\n",
      "DataFrame from list of dicts:\n",
      "      name  age      city  salary\n",
      "0    Alice   25  New York   75000\n",
      "1      Bob   30    London   82000\n",
      "2  Charlie   35     Tokyo   68000\n",
      "3    Diana   28     Paris   71000\n",
      "Shape: (4, 4)\n",
      "Data types:\n",
      "name      object\n",
      "age        int64\n",
      "city      object\n",
      "salary     int64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1.1 Creating DataFrame from LIST of dictionaries\n",
    "print(\"🔹 Creating DataFrame from LIST of dictionaries:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# List of dictionaries - most common way\n",
    "data_list = [\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\", \"salary\": 75000},\n",
    "    {\"name\": \"Bob\", \"age\": 30, \"city\": \"London\", \"salary\": 82000},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Tokyo\", \"salary\": 68000},\n",
    "    {\"name\": \"Diana\", \"age\": 28, \"city\": \"Paris\", \"salary\": 71000},\n",
    "]\n",
    "\n",
    "df_from_list = pd.DataFrame(data_list)\n",
    "print(\"DataFrame from list of dicts:\")\n",
    "print(df_from_list)\n",
    "print(f\"Shape: {df_from_list.shape}\")\n",
    "print(f\"Data types:\\n{df_from_list.dtypes}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274c45ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Creating DataFrame from DICTIONARY of lists:\n",
      "--------------------------------------------------\n",
      "DataFrame from dict of lists:\n",
      "      product   price     category  in_stock  rating\n",
      "0      Laptop  999.99  Electronics      True     4.5\n",
      "1       Mouse   29.99  Accessories      True     4.2\n",
      "2    Keyboard   79.99  Accessories     False     4.8\n",
      "3     Monitor  299.99  Electronics      True     4.3\n",
      "4  Headphones  149.99        Audio      True     4.6\n",
      "Shape: (5, 5)\n",
      "Data types:\n",
      "product      object\n",
      "price       float64\n",
      "category     object\n",
      "in_stock       bool\n",
      "rating      float64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1.2 Creating DataFrame from DICTIONARY of lists\n",
    "print(\"🔹 Creating DataFrame from DICTIONARY of lists:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Dictionary of lists - efficient for large datasets\n",
    "data_dict = {\n",
    "    \"product\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Headphones\"],\n",
    "    \"price\": [999.99, 29.99, 79.99, 299.99, 149.99],\n",
    "    \"category\": [\"Electronics\", \"Accessories\", \"Accessories\", \"Electronics\", \"Audio\"],\n",
    "    \"in_stock\": [True, True, False, True, True],\n",
    "    \"rating\": [4.5, 4.2, 4.8, 4.3, 4.6],\n",
    "}\n",
    "\n",
    "df_from_dict = pd.DataFrame(data_dict)\n",
    "print(\"DataFrame from dict of lists:\")\n",
    "print(df_from_dict)\n",
    "print(f\"Shape: {df_from_dict.shape}\")\n",
    "print(f\"Data types:\\n{df_from_dict.dtypes}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400d38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Creating DataFrame from TUPLE of tuples:\n",
      "--------------------------------------------------\n",
      "DataFrame from tuple of tuples:\n",
      "  first_name last_name  age  job_title  salary\n",
      "0       John       Doe   32   Engineer   85000\n",
      "1       Jane     Smith   28   Designer   72000\n",
      "2       Mike   Johnson   35    Manager   95000\n",
      "3      Sarah    Wilson   29    Analyst   68000\n",
      "4        Tom     Brown   31  Developer   78000\n",
      "Shape: (5, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1.3 Creating DataFrame from TUPLE of tuples (2D data)\n",
    "print(\"🔹 Creating DataFrame from TUPLE of tuples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Tuple of tuples - immutable data structure\n",
    "data_tuple = (\n",
    "    (\"John\", \"Doe\", 32, \"Engineer\", 85000),\n",
    "    (\"Jane\", \"Smith\", 28, \"Designer\", 72000),\n",
    "    (\"Mike\", \"Johnson\", 35, \"Manager\", 95000),\n",
    "    (\"Sarah\", \"Wilson\", 29, \"Analyst\", 68000),\n",
    "    (\"Tom\", \"Brown\", 31, \"Developer\", 78000),\n",
    ")\n",
    "\n",
    "columns = [\"first_name\", \"last_name\", \"age\", \"job_title\", \"salary\"]\n",
    "df_from_tuple = pd.DataFrame(data_tuple, columns=columns)\n",
    "print(\"DataFrame from tuple of tuples:\")\n",
    "print(df_from_tuple)\n",
    "print(f\"Shape: {df_from_tuple.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9acdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Creating DataFrame using SET operations:\n",
      "--------------------------------------------------\n",
      "DataFrame from set operations:\n",
      "    department  employee_count  budget  budget_per_employee\n",
      "0           HR              15  272475         18165.000000\n",
      "1  Engineering               8  281207         35150.875000\n",
      "2    Marketing              11  294439         26767.181818\n",
      "3        Sales               8  490094         61261.750000\n",
      "4      Finance               7  494990         70712.857143\n",
      "Shape: (5, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1.4 Creating DataFrame from SET operations and comprehensions\n",
    "print(\"🔹 Creating DataFrame using SET operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Using set comprehensions and operations\n",
    "departments = {\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"}\n",
    "employees_per_dept = {dept: random.randint(5, 20) for dept in departments}\n",
    "budget_per_dept = {dept: random.randint(100000, 500000) for dept in departments}\n",
    "\n",
    "# Convert sets to DataFrames\n",
    "dept_data = []\n",
    "for dept in departments:\n",
    "    dept_data.append(\n",
    "        {\n",
    "            \"department\": dept,\n",
    "            \"employee_count\": employees_per_dept[dept],\n",
    "            \"budget\": budget_per_dept[dept],\n",
    "            \"budget_per_employee\": budget_per_dept[dept] / employees_per_dept[dept],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_from_set = pd.DataFrame(dept_data)\n",
    "print(\"DataFrame from set operations:\")\n",
    "print(df_from_set)\n",
    "print(f\"Shape: {df_from_set.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80baa47c",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Basic DataFrame Operations\n",
    "\n",
    "Now let's practice fundamental DataFrame operations using our created data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cd48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Basic DataFrame Operations:\n",
      "--------------------------------------------------\n",
      "Original DataFrame:\n",
      "  first_name last_name  age  job_title  salary\n",
      "0       John       Doe   32   Engineer   85000\n",
      "1       Jane     Smith   28   Designer   72000\n",
      "2       Mike   Johnson   35    Manager   95000\n",
      "3      Sarah    Wilson   29    Analyst   68000\n",
      "4        Tom     Brown   31  Developer   78000\n",
      "\n",
      "📊 DataFrame Info:\n",
      "Shape: (5, 5)\n",
      "Columns: ['first_name', 'last_name', 'age', 'job_title', 'salary']\n",
      "Index: [0, 1, 2, 3, 4]\n",
      "Data types:\n",
      "first_name    object\n",
      "last_name     object\n",
      "age            int64\n",
      "job_title     object\n",
      "salary         int64\n",
      "dtype: object\n",
      "\n",
      "📋 Selection Operations:\n",
      "First 3 rows:\n",
      "  first_name last_name  age job_title  salary\n",
      "0       John       Doe   32  Engineer   85000\n",
      "1       Jane     Smith   28  Designer   72000\n",
      "2       Mike   Johnson   35   Manager   95000\n",
      "\n",
      "Last 2 rows:\n",
      "  first_name last_name  age  job_title  salary\n",
      "3      Sarah    Wilson   29    Analyst   68000\n",
      "4        Tom     Brown   31  Developer   78000\n",
      "\n",
      "Specific columns:\n",
      "  first_name  salary\n",
      "0       John   85000\n",
      "1       Jane   72000\n",
      "2       Mike   95000\n",
      "3      Sarah   68000\n",
      "4        Tom   78000\n",
      "\n",
      "Specific rows and columns:\n",
      "  first_name job_title  salary\n",
      "1       Jane  Designer   72000\n",
      "2       Mike   Manager   95000\n",
      "3      Sarah   Analyst   68000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2.1 Basic DataFrame inspection and selection\n",
    "print(\"🔹 Basic DataFrame Operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Using our employee data\n",
    "df = df_from_tuple.copy()\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Basic inspection methods\n",
    "print(\"📊 DataFrame Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Index: {list(df.index)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "print()\n",
    "\n",
    "# Selection operations\n",
    "print(\"📋 Selection Operations:\")\n",
    "print(\"First 3 rows:\")\n",
    "print(df.head(3))\n",
    "print()\n",
    "\n",
    "print(\"Last 2 rows:\")\n",
    "print(df.tail(2))\n",
    "print()\n",
    "\n",
    "print(\"Specific columns:\")\n",
    "print(df[[\"first_name\", \"salary\"]])\n",
    "print()\n",
    "\n",
    "print(\"Specific rows and columns:\")\n",
    "print(df.loc[1:3, [\"first_name\", \"job_title\", \"salary\"]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c896d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Adding and Modifying Columns:\n",
      "--------------------------------------------------\n",
      "DataFrame with new columns:\n",
      "  first_name last_name  age  job_title  salary     full_name salary_category  \\\n",
      "0       John       Doe   32   Engineer   85000      John Doe            High   \n",
      "1       Jane     Smith   28   Designer   72000    Jane Smith          Medium   \n",
      "2       Mike   Johnson   35    Manager   95000  Mike Johnson            High   \n",
      "3      Sarah    Wilson   29    Analyst   68000  Sarah Wilson             Low   \n",
      "4        Tom     Brown   31  Developer   78000     Tom Brown          Medium   \n",
      "\n",
      "   years_until_retirement  \n",
      "0                      33  \n",
      "1                      37  \n",
      "2                      30  \n",
      "3                      36  \n",
      "4                      34  \n",
      "\n",
      "After 10% salary raise:\n",
      "      full_name    salary salary_category\n",
      "0      John Doe   93500.0            High\n",
      "1    Jane Smith   79200.0          Medium\n",
      "2  Mike Johnson  104500.0            High\n",
      "3  Sarah Wilson   74800.0             Low\n",
      "4     Tom Brown   85800.0          Medium\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2.2 Adding and modifying columns\n",
    "print(\"🔹 Adding and Modifying Columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Add new columns\n",
    "df[\"full_name\"] = df[\"first_name\"] + \" \" + df[\"last_name\"]\n",
    "df[\"salary_category\"] = df[\"salary\"].apply(lambda x: \"High\" if x > 80000 else \"Medium\" if x > 70000 else \"Low\")\n",
    "df[\"years_until_retirement\"] = 65 - df[\"age\"]\n",
    "\n",
    "print(\"DataFrame with new columns:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Modify existing columns\n",
    "df[\"salary\"] = df[\"salary\"] * 1.1  # 10% raise\n",
    "print(\"After 10% salary raise:\")\n",
    "print(df[[\"full_name\", \"salary\", \"salary_category\"]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063217e6",
   "metadata": {},
   "source": [
    "# 🎯 LEVEL 2: INTERMEDIATE PANDAS OPERATIONS\n",
    "\n",
    "## Exercise 2.1: Data Filtering and Conditional Operations\n",
    "\n",
    "Let's dive deeper into data manipulation using various filtering techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b992d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Advanced Filtering Operations:\n",
      "--------------------------------------------------\n",
      "Employee Dataset:\n",
      "  employee_id     name  age   department      city  salary  years_experience  \\\n",
      "0      EMP001    Alice   44        Sales     Tokyo   96901                 1   \n",
      "1      EMP002      Bob   24           HR    Sydney  111275                 8   \n",
      "2      EMP003  Charlie   61      Finance     Paris  102492                16   \n",
      "3      EMP004    Diana   60        Sales    Berlin   86002                 3   \n",
      "4      EMP005      Eve   46  Engineering     Tokyo   56586                11   \n",
      "5      EMP006    Frank   42    Marketing    Berlin  113046                 5   \n",
      "6      EMP007    Grace   30      Finance     Paris   79537                19   \n",
      "7      EMP008    Henry   57           HR  New York  119664                16   \n",
      "8      EMP009      Ivy   47    Marketing     Tokyo   67366                16   \n",
      "9      EMP010     Jack   37  Engineering    London   50843                 1   \n",
      "\n",
      "   performance_score  is_manager  \n",
      "0                1.3        True  \n",
      "1                3.7        True  \n",
      "2                3.2       False  \n",
      "3                3.2       False  \n",
      "4                3.5       False  \n",
      "5                3.2        True  \n",
      "6                4.6        True  \n",
      "7                1.7       False  \n",
      "8                4.3       False  \n",
      "9                1.4       False  \n",
      "Total employees: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1.1 Advanced filtering with multiple conditions\n",
    "print(\"🔹 Advanced Filtering Operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a larger dataset for better filtering examples\n",
    "np.random.seed(42)\n",
    "n_employees = 20\n",
    "\n",
    "# Generate employee data using different data structures\n",
    "first_names = [\n",
    "    \"Alice\",\n",
    "    \"Bob\",\n",
    "    \"Charlie\",\n",
    "    \"Diana\",\n",
    "    \"Eve\",\n",
    "    \"Frank\",\n",
    "    \"Grace\",\n",
    "    \"Henry\",\n",
    "    \"Ivy\",\n",
    "    \"Jack\",\n",
    "    \"Kate\",\n",
    "    \"Liam\",\n",
    "    \"Maya\",\n",
    "    \"Noah\",\n",
    "    \"Olivia\",\n",
    "    \"Paul\",\n",
    "    \"Quinn\",\n",
    "    \"Ruby\",\n",
    "    \"Sam\",\n",
    "    \"Tina\",\n",
    "]\n",
    "\n",
    "departments = [\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"]\n",
    "cities = [\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Berlin\", \"Sydney\"]\n",
    "\n",
    "# Create comprehensive employee data\n",
    "employee_data = []\n",
    "for i in range(n_employees):\n",
    "    employee_data.append(\n",
    "        {\n",
    "            \"employee_id\": f\"EMP{i + 1:03d}\",\n",
    "            \"name\": first_names[i],\n",
    "            \"age\": random.randint(22, 65),\n",
    "            \"department\": random.choice(departments),\n",
    "            \"city\": random.choice(cities),\n",
    "            \"salary\": random.randint(40000, 120000),\n",
    "            \"years_experience\": random.randint(0, 20),\n",
    "            \"performance_score\": round(random.uniform(1.0, 5.0), 1),\n",
    "            \"is_manager\": random.choice([True, False]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_employees = pd.DataFrame(employee_data)\n",
    "print(\"Employee Dataset:\")\n",
    "print(df_employees.head(10))\n",
    "print(f\"Total employees: {len(df_employees)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075aed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Complex Filtering Examples:\n",
      "--------------------------------------------------\n",
      "High performers in Engineering:\n",
      "Empty DataFrame\n",
      "Columns: [name, department, performance_score, salary]\n",
      "Index: []\n",
      "\n",
      "Senior employees (age > 40 OR experience > 10 years):\n",
      "       name  age  years_experience   department\n",
      "0     Alice   44                 1        Sales\n",
      "2   Charlie   61                16      Finance\n",
      "3     Diana   60                 3        Sales\n",
      "4       Eve   46                11  Engineering\n",
      "5     Frank   42                 5    Marketing\n",
      "6     Grace   30                19      Finance\n",
      "7     Henry   57                16           HR\n",
      "8       Ivy   47                16    Marketing\n",
      "10     Kate   62                18      Finance\n",
      "11     Liam   37                20        Sales\n",
      "12     Maya   56                 5        Sales\n",
      "13     Noah   22                12    Marketing\n",
      "14   Olivia   51                18        Sales\n",
      "15     Paul   61                16           HR\n",
      "16    Quinn   23                13      Finance\n",
      "17     Ruby   45                15        Sales\n",
      "18      Sam   53                 5      Finance\n",
      "\n",
      "Employees in major cities:\n",
      "    name      city   department\n",
      "0  Alice     Tokyo        Sales\n",
      "4    Eve     Tokyo  Engineering\n",
      "7  Henry  New York           HR\n",
      "8    Ivy     Tokyo    Marketing\n",
      "9   Jack    London  Engineering\n",
      "\n",
      "High earners with good performance:\n",
      "      name  salary  performance_score department\n",
      "1      Bob  111275                3.7         HR\n",
      "14  Olivia  109504                4.5      Sales\n",
      "16   Quinn   90314                4.0    Finance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1.2 Complex filtering with multiple conditions\n",
    "print(\"🔹 Complex Filtering Examples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Filter 1: High performers in Engineering\n",
    "high_performers_eng = df_employees[(df_employees[\"department\"] == \"Engineering\") & (df_employees[\"performance_score\"] >= 4.0)]\n",
    "print(\"High performers in Engineering:\")\n",
    "print(high_performers_eng[[\"name\", \"department\", \"performance_score\", \"salary\"]])\n",
    "print()\n",
    "\n",
    "# Filter 2: Senior employees (age > 40 OR experience > 10 years)\n",
    "senior_employees = df_employees[(df_employees[\"age\"] > 40) | (df_employees[\"years_experience\"] > 10)]\n",
    "print(\"Senior employees (age > 40 OR experience > 10 years):\")\n",
    "print(senior_employees[[\"name\", \"age\", \"years_experience\", \"department\"]])\n",
    "print()\n",
    "\n",
    "# Filter 3: Using isin() with lists\n",
    "target_cities = [\"New York\", \"London\", \"Tokyo\"]\n",
    "employees_in_major_cities = df_employees[df_employees[\"city\"].isin(target_cities)]\n",
    "print(\"Employees in major cities:\")\n",
    "print(employees_in_major_cities[[\"name\", \"city\", \"department\"]].head())\n",
    "print()\n",
    "\n",
    "# Filter 4: Using query() method for complex conditions\n",
    "high_earners = df_employees.query(\"salary > 80000 and performance_score >= 3.5\")\n",
    "print(\"High earners with good performance:\")\n",
    "print(high_earners[[\"name\", \"salary\", \"performance_score\", \"department\"]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74b2f2",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Grouping and Aggregation Operations\n",
    "\n",
    "Master the art of grouping data and performing aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d33c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Grouping and Aggregation Operations:\n",
      "--------------------------------------------------\n",
      "Department Statistics:\n",
      "                salary                                       age          \\\n",
      "                  mean    median       std    min     max   mean min max   \n",
      "department                                                                 \n",
      "Engineering   53714.50   53714.5   4060.91  50843   56586  41.50  37  46   \n",
      "Finance       81798.00   79537.0  15430.69  61865  102492  45.80  23  62   \n",
      "HR           103821.67  111275.0  20606.06  80526  119664  47.33  24  61   \n",
      "Marketing     75990.25   75044.5  30162.67  40826  113046  37.25  22  47   \n",
      "Sales         75662.83   78660.0  26883.50  41482  109504  48.83  37  60   \n",
      "\n",
      "            performance_score       is_manager  \n",
      "                         mean count        sum  \n",
      "department                                      \n",
      "Engineering              2.45     2          0  \n",
      "Finance                  3.60     5          4  \n",
      "HR                       2.70     3          2  \n",
      "Marketing                3.00     4          2  \n",
      "Sales                    2.90     6          4  \n",
      "\n",
      "Flattened column names:\n",
      "             salary_mean  salary_median  salary_std  salary_min  salary_max  \\\n",
      "department                                                                    \n",
      "Engineering     53714.50        53714.5     4060.91       50843       56586   \n",
      "Finance         81798.00        79537.0    15430.69       61865      102492   \n",
      "HR             103821.67       111275.0    20606.06       80526      119664   \n",
      "Marketing       75990.25        75044.5    30162.67       40826      113046   \n",
      "Sales           75662.83        78660.0    26883.50       41482      109504   \n",
      "\n",
      "             age_mean  age_min  age_max  performance_score_mean  \\\n",
      "department                                                        \n",
      "Engineering     41.50       37       46                    2.45   \n",
      "Finance         45.80       23       62                    3.60   \n",
      "HR              47.33       24       61                    2.70   \n",
      "Marketing       37.25       22       47                    3.00   \n",
      "Sales           48.83       37       60                    2.90   \n",
      "\n",
      "             performance_score_count  is_manager_sum  \n",
      "department                                            \n",
      "Engineering                        2               0  \n",
      "Finance                            5               4  \n",
      "HR                                 3               2  \n",
      "Marketing                          4               2  \n",
      "Sales                              6               4  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2.1 Basic grouping operations\n",
    "print(\"🔹 Grouping and Aggregation Operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Group by department and calculate statistics\n",
    "dept_stats = (\n",
    "    df_employees.groupby(\"department\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"salary\": [\"mean\", \"median\", \"std\", \"min\", \"max\"],\n",
    "            \"age\": [\"mean\", \"min\", \"max\"],\n",
    "            \"performance_score\": [\"mean\", \"count\"],\n",
    "            \"is_manager\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"Department Statistics:\")\n",
    "print(dept_stats)\n",
    "print()\n",
    "\n",
    "# Flatten column names for better readability\n",
    "dept_stats.columns = [\"_\".join(col).strip() for col in dept_stats.columns]\n",
    "print(\"Flattened column names:\")\n",
    "print(dept_stats)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae4674d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Multi-level Grouping:\n",
      "--------------------------------------------------\n",
      "Statistics by City and Department:\n",
      "                      avg_salary  avg_performance  employee_count\n",
      "city     department                                              \n",
      "Berlin   Marketing       76936.0             2.75               2\n",
      "         Sales           78660.0             2.20               2\n",
      "London   Engineering     50843.0             1.40               1\n",
      "         Marketing       82723.0             2.20               1\n",
      "         Sales          109504.0             4.50               1\n",
      "New York Finance         68323.5             3.10               2\n",
      "         HR             100095.0             2.20               2\n",
      "         Sales           41482.0             2.90               1\n",
      "Paris    Finance         91014.5             3.90               2\n",
      "Sydney   HR             111275.0             3.70               1\n",
      "Tokyo    Engineering     56586.0             3.50               1\n",
      "         Finance         90314.0             4.00               1\n",
      "         Marketing       67366.0             4.30               1\n",
      "         Sales           72835.5             2.80               2\n",
      "\n",
      "Custom Aggregations:\n",
      "                  salary             performance_score      \n",
      "            salary_range       mean  performance_grade  mean\n",
      "department                                                  \n",
      "Engineering         5743   53714.50  Needs Improvement  2.45\n",
      "Finance            40627   81798.00               Good  3.60\n",
      "HR                 39138  103821.67            Average  2.70\n",
      "Marketing          72220   75990.25            Average  3.00\n",
      "Sales              68022   75662.83            Average  2.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2.2 Multi-level grouping and custom aggregations\n",
    "print(\"🔹 Multi-level Grouping:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Group by multiple columns\n",
    "city_dept_stats = (\n",
    "    df_employees.groupby([\"city\", \"department\"])\n",
    "    .agg({\"salary\": \"mean\", \"performance_score\": \"mean\", \"employee_id\": \"count\"})\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "city_dept_stats.columns = [\"avg_salary\", \"avg_performance\", \"employee_count\"]\n",
    "print(\"Statistics by City and Department:\")\n",
    "print(city_dept_stats)\n",
    "print()\n",
    "\n",
    "# Custom aggregation functions\n",
    "\n",
    "\n",
    "def salary_range(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "\n",
    "def performance_grade(series):\n",
    "    avg_score = series.mean()\n",
    "    if avg_score >= 4.5:\n",
    "        return \"Excellent\"\n",
    "    elif avg_score >= 3.5:\n",
    "        return \"Good\"\n",
    "    elif avg_score >= 2.5:\n",
    "        return \"Average\"\n",
    "    else:\n",
    "        return \"Needs Improvement\"\n",
    "\n",
    "\n",
    "custom_stats = (\n",
    "    df_employees.groupby(\"department\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"salary\": [salary_range, \"mean\"],\n",
    "            \"performance_score\": [performance_grade, \"mean\"],\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"Custom Aggregations:\")\n",
    "print(custom_stats)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe821a6",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Data Transformation with Different Data Types\n",
    "\n",
    "Let's explore how to work with various Python data types within pandas operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2b895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Working with Lists in DataFrame:\n",
      "--------------------------------------------------\n",
      "Skills DataFrame:\n",
      "  employee_id     name                 programming_languages  \\\n",
      "0      EMP001    Alice                   [Python, Java, SQL]   \n",
      "1      EMP002      Bob           [JavaScript, Python, React]   \n",
      "2      EMP003  Charlie    [Python, R, SQL, Machine Learning]   \n",
      "3      EMP004    Diana                 [Java, Spring, MySQL]   \n",
      "4      EMP005      Eve  [Python, Django, PostgreSQL, Docker]   \n",
      "\n",
      "                     certifications  \n",
      "0               [AWS, Google Cloud]  \n",
      "1                  [React, Node.js]  \n",
      "2  [Data Science, Machine Learning]  \n",
      "3            [Java SE, Spring Boot]  \n",
      "4      [Python, Docker, Kubernetes]  \n",
      "\n",
      "With skill counts:\n",
      "      name  num_languages  num_certifications\n",
      "0    Alice              3                   2\n",
      "1      Bob              3                   2\n",
      "2  Charlie              4                   2\n",
      "3    Diana              3                   2\n",
      "4      Eve              4                   3\n",
      "\n",
      "Skill presence check:\n",
      "      name  knows_python  knows_java\n",
      "0    Alice          True        True\n",
      "1      Bob          True       False\n",
      "2  Charlie          True       False\n",
      "3    Diana         False        True\n",
      "4      Eve          True       False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3.1 Working with Lists in DataFrame columns\n",
    "print(\"🔹 Working with Lists in DataFrame:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a DataFrame with list columns\n",
    "skills_data = {\n",
    "    \"employee_id\": [\"EMP001\", \"EMP002\", \"EMP003\", \"EMP004\", \"EMP005\"],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n",
    "    \"programming_languages\": [\n",
    "        [\"Python\", \"Java\", \"SQL\"],\n",
    "        [\"JavaScript\", \"Python\", \"React\"],\n",
    "        [\"Python\", \"R\", \"SQL\", \"Machine Learning\"],\n",
    "        [\"Java\", \"Spring\", \"MySQL\"],\n",
    "        [\"Python\", \"Django\", \"PostgreSQL\", \"Docker\"],\n",
    "    ],\n",
    "    \"certifications\": [\n",
    "        [\"AWS\", \"Google Cloud\"],\n",
    "        [\"React\", \"Node.js\"],\n",
    "        [\"Data Science\", \"Machine Learning\"],\n",
    "        [\"Java SE\", \"Spring Boot\"],\n",
    "        [\"Python\", \"Docker\", \"Kubernetes\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_skills = pd.DataFrame(skills_data)\n",
    "print(\"Skills DataFrame:\")\n",
    "print(df_skills)\n",
    "print()\n",
    "\n",
    "# Count skills per employee\n",
    "df_skills[\"num_languages\"] = df_skills[\"programming_languages\"].apply(len)\n",
    "df_skills[\"num_certifications\"] = df_skills[\"certifications\"].apply(len)\n",
    "print(\"With skill counts:\")\n",
    "print(df_skills[[\"name\", \"num_languages\", \"num_certifications\"]])\n",
    "print()\n",
    "\n",
    "# Check if employee has specific skills\n",
    "df_skills[\"knows_python\"] = df_skills[\"programming_languages\"].apply(lambda x: \"Python\" in x)\n",
    "df_skills[\"knows_java\"] = df_skills[\"programming_languages\"].apply(lambda x: \"Java\" in x)\n",
    "print(\"Skill presence check:\")\n",
    "print(df_skills[[\"name\", \"knows_python\", \"knows_java\"]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a276a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Working with Dictionaries in DataFrame:\n",
      "--------------------------------------------------\n",
      "Employee Details DataFrame:\n",
      "  employee_id     name                                       contact_info  \\\n",
      "0      EMP001    Alice  {'email': 'alice@company.com', 'phone': '+1-55...   \n",
      "1      EMP002      Bob  {'email': 'bob@company.com', 'phone': '+1-555-...   \n",
      "2      EMP003  Charlie  {'email': 'charlie@company.com', 'phone': '+1-...   \n",
      "3      EMP004    Diana  {'email': 'diana@company.com', 'phone': '+1-55...   \n",
      "4      EMP005      Eve  {'email': 'eve@company.com', 'phone': '+1-555-...   \n",
      "\n",
      "                                 performance_metrics  \n",
      "0  {'projects_completed': 12, 'client_satisfactio...  \n",
      "1  {'projects_completed': 8, 'client_satisfaction...  \n",
      "2  {'projects_completed': 15, 'client_satisfactio...  \n",
      "3  {'projects_completed': 10, 'client_satisfactio...  \n",
      "4  {'projects_completed': 13, 'client_satisfactio...  \n",
      "\n",
      "Extracted information:\n",
      "      name                email        phone  projects_completed  \\\n",
      "0    Alice    alice@company.com  +1-555-0101                  12   \n",
      "1      Bob      bob@company.com  +1-555-0102                   8   \n",
      "2  Charlie  charlie@company.com  +1-555-0103                  15   \n",
      "3    Diana    diana@company.com  +1-555-0104                  10   \n",
      "4      Eve      eve@company.com  +1-555-0105                  13   \n",
      "\n",
      "   client_satisfaction  \n",
      "0                  4.8  \n",
      "1                  4.2  \n",
      "2                  4.9  \n",
      "3                  4.6  \n",
      "4                  4.7  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3.2 Working with Dictionaries in DataFrame columns\n",
    "print(\"🔹 Working with Dictionaries in DataFrame:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create DataFrame with dictionary columns\n",
    "employee_details = {\n",
    "    \"employee_id\": [\"EMP001\", \"EMP002\", \"EMP003\", \"EMP004\", \"EMP005\"],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n",
    "    \"contact_info\": [\n",
    "        {\n",
    "            \"email\": \"alice@company.com\",\n",
    "            \"phone\": \"+1-555-0101\",\n",
    "            \"address\": \"123 Main St\",\n",
    "        },\n",
    "        {\"email\": \"bob@company.com\", \"phone\": \"+1-555-0102\", \"address\": \"456 Oak Ave\"},\n",
    "        {\n",
    "            \"email\": \"charlie@company.com\",\n",
    "            \"phone\": \"+1-555-0103\",\n",
    "            \"address\": \"789 Pine Rd\",\n",
    "        },\n",
    "        {\"email\": \"diana@company.com\", \"phone\": \"+1-555-0104\", \"address\": \"321 Elm St\"},\n",
    "        {\"email\": \"eve@company.com\", \"phone\": \"+1-555-0105\", \"address\": \"654 Maple Dr\"},\n",
    "    ],\n",
    "    \"performance_metrics\": [\n",
    "        {\n",
    "            \"projects_completed\": 12,\n",
    "            \"client_satisfaction\": 4.8,\n",
    "            \"team_collaboration\": 4.5,\n",
    "        },\n",
    "        {\n",
    "            \"projects_completed\": 8,\n",
    "            \"client_satisfaction\": 4.2,\n",
    "            \"team_collaboration\": 4.7,\n",
    "        },\n",
    "        {\n",
    "            \"projects_completed\": 15,\n",
    "            \"client_satisfaction\": 4.9,\n",
    "            \"team_collaboration\": 4.3,\n",
    "        },\n",
    "        {\n",
    "            \"projects_completed\": 10,\n",
    "            \"client_satisfaction\": 4.6,\n",
    "            \"team_collaboration\": 4.8,\n",
    "        },\n",
    "        {\n",
    "            \"projects_completed\": 13,\n",
    "            \"client_satisfaction\": 4.7,\n",
    "            \"team_collaboration\": 4.4,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_details = pd.DataFrame(employee_details)\n",
    "print(\"Employee Details DataFrame:\")\n",
    "print(df_details)\n",
    "print()\n",
    "\n",
    "# Extract specific values from dictionaries\n",
    "df_details[\"email\"] = df_details[\"contact_info\"].apply(lambda x: x[\"email\"])\n",
    "df_details[\"phone\"] = df_details[\"contact_info\"].apply(lambda x: x[\"phone\"])\n",
    "df_details[\"projects_completed\"] = df_details[\"performance_metrics\"].apply(lambda x: x[\"projects_completed\"])\n",
    "df_details[\"client_satisfaction\"] = df_details[\"performance_metrics\"].apply(lambda x: x[\"client_satisfaction\"])\n",
    "\n",
    "print(\"Extracted information:\")\n",
    "print(df_details[[\"name\", \"email\", \"phone\", \"projects_completed\", \"client_satisfaction\"]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "464bdc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Working with Sets and Tuples:\n",
      "--------------------------------------------------\n",
      "Projects DataFrame:\n",
      "  project_id         project_name  \\\n",
      "0    PROJ001  E-commerce Platform   \n",
      "1    PROJ002           Mobile App   \n",
      "2    PROJ003       Data Analytics   \n",
      "3    PROJ004           AI Chatbot   \n",
      "4    PROJ005      Cloud Migration   \n",
      "\n",
      "                              technologies_used                team_members  \\\n",
      "0           {Django, Python, PostgreSQL, Redis}       (Alice, Bob, Charlie)   \n",
      "1          {Firebase, React Native, JavaScript}                (Diana, Eve)   \n",
      "2  {NumPy, Matplotlib, Python, Pandas, Jupyter}  (Frank, Grace, Henry, Ivy)   \n",
      "3             {NLTK, TensorFlow, Python, Flask}                (Jack, Kate)   \n",
      "4          {Kubernetes, AWS, Docker, Terraform}          (Liam, Maya, Noah)   \n",
      "\n",
      "  project_duration  \n",
      "0      (6, months)  \n",
      "1      (4, months)  \n",
      "2      (8, months)  \n",
      "3      (5, months)  \n",
      "4     (10, months)  \n",
      "\n",
      "Project Analysis:\n",
      "          project_name  num_technologies  num_team_members  duration_months\n",
      "0  E-commerce Platform                 4                 3                6\n",
      "1           Mobile App                 3                 2                4\n",
      "2       Data Analytics                 5                 4                8\n",
      "3           AI Chatbot                 4                 2                5\n",
      "4      Cloud Migration                 4                 3               10\n",
      "\n",
      "All unique technologies used: ['AWS', 'Django', 'Docker', 'Firebase', 'Flask', 'JavaScript', 'Jupyter', 'Kubernetes', 'Matplotlib', 'NLTK', 'NumPy', 'Pandas', 'PostgreSQL', 'Python', 'React Native', 'Redis', 'TensorFlow', 'Terraform']\n",
      "\n",
      "Projects using Python:\n",
      "          project_name                             technologies_used\n",
      "0  E-commerce Platform           {Django, Python, PostgreSQL, Redis}\n",
      "2       Data Analytics  {NumPy, Matplotlib, Python, Pandas, Jupyter}\n",
      "3           AI Chatbot             {NLTK, TensorFlow, Python, Flask}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3.3 Working with Sets and Tuples\n",
    "print(\"🔹 Working with Sets and Tuples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create DataFrame with set and tuple columns\n",
    "project_data = {\n",
    "    \"project_id\": [\"PROJ001\", \"PROJ002\", \"PROJ003\", \"PROJ004\", \"PROJ005\"],\n",
    "    \"project_name\": [\n",
    "        \"E-commerce Platform\",\n",
    "        \"Mobile App\",\n",
    "        \"Data Analytics\",\n",
    "        \"AI Chatbot\",\n",
    "        \"Cloud Migration\",\n",
    "    ],\n",
    "    \"technologies_used\": [\n",
    "        {\"Python\", \"Django\", \"PostgreSQL\", \"Redis\"},\n",
    "        {\"React Native\", \"JavaScript\", \"Firebase\"},\n",
    "        {\"Python\", \"Pandas\", \"NumPy\", \"Matplotlib\", \"Jupyter\"},\n",
    "        {\"Python\", \"TensorFlow\", \"NLTK\", \"Flask\"},\n",
    "        {\"AWS\", \"Docker\", \"Kubernetes\", \"Terraform\"},\n",
    "    ],\n",
    "    \"team_members\": [\n",
    "        (\"Alice\", \"Bob\", \"Charlie\"),\n",
    "        (\"Diana\", \"Eve\"),\n",
    "        (\"Frank\", \"Grace\", \"Henry\", \"Ivy\"),\n",
    "        (\"Jack\", \"Kate\"),\n",
    "        (\"Liam\", \"Maya\", \"Noah\"),\n",
    "    ],\n",
    "    \"project_duration\": [\n",
    "        (6, \"months\"),\n",
    "        (4, \"months\"),\n",
    "        (8, \"months\"),\n",
    "        (5, \"months\"),\n",
    "        (10, \"months\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_projects = pd.DataFrame(project_data)\n",
    "print(\"Projects DataFrame:\")\n",
    "print(df_projects)\n",
    "print()\n",
    "\n",
    "# Analyze technologies\n",
    "df_projects[\"num_technologies\"] = df_projects[\"technologies_used\"].apply(len)\n",
    "df_projects[\"num_team_members\"] = df_projects[\"team_members\"].apply(len)\n",
    "df_projects[\"duration_months\"] = df_projects[\"project_duration\"].apply(lambda x: x[0])\n",
    "\n",
    "print(\"Project Analysis:\")\n",
    "print(df_projects[[\"project_name\", \"num_technologies\", \"num_team_members\", \"duration_months\"]])\n",
    "print()\n",
    "\n",
    "# Find common technologies across projects\n",
    "all_technologies = set()\n",
    "for tech_set in df_projects[\"technologies_used\"]:\n",
    "    all_technologies.update(tech_set)\n",
    "\n",
    "print(f\"All unique technologies used: {sorted(all_technologies)}\")\n",
    "print()\n",
    "\n",
    "# Check which projects use Python\n",
    "python_projects = df_projects[df_projects[\"technologies_used\"].apply(lambda x: \"Python\" in x)]\n",
    "print(\"Projects using Python:\")\n",
    "print(python_projects[[\"project_name\", \"technologies_used\"]])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b038ff",
   "metadata": {},
   "source": [
    "# 🚀 LEVEL 3: ADVANCED PANDAS OPERATIONS\n",
    "\n",
    "## Exercise 3.1: Advanced Data Manipulation and Performance Optimization\n",
    "\n",
    "Let's explore advanced pandas features and optimization techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c0dc779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Advanced Indexing and MultiIndex:\n",
      "--------------------------------------------------\n",
      "Sales DataFrame with MultiIndex:\n",
      "                              sales       price  quantity       revenue\n",
      "date       product    region                                           \n",
      "2023-01-01 Headphones East       24  280.804934        19   6739.318414\n",
      "                      North      75  240.912321        18  18068.424045\n",
      "                      South      64  987.705276         2  63213.137666\n",
      "                      West       54  148.568788        19   8022.714539\n",
      "           Keyboard   East       37  638.064548         5  23608.388261\n",
      "                      North      84  739.087640         3  62083.361746\n",
      "                      South      57  386.616074         5  22037.116228\n",
      "                      West       87  529.419202         1  46059.470542\n",
      "           Laptop     East       43  814.401850        16  35019.279556\n",
      "                      North      72  694.820345        12  50027.064809\n",
      "\n",
      "Sales for Laptop in North region:\n",
      "                            sales       price  quantity       revenue\n",
      "date       product  region                                           \n",
      "2023-01-01 Laptop   North      72  694.820345        12  50027.064809\n",
      "                    South      85  548.691221        17  46638.753790\n",
      "                    West       65  458.953513        10  29831.978337\n",
      "           Monitor  East       96  164.391610         2  15781.594551\n",
      "                    North      58  659.920321        12  38275.378609\n",
      "...                           ...         ...       ...           ...\n",
      "2023-01-05 Keyboard North      70   75.979877        20   5318.591371\n",
      "                    South      42  285.971306        17  12010.794834\n",
      "                    West       46  305.132789         1  14036.108315\n",
      "           Laptop   East       45  965.932551        12  43466.964817\n",
      "                    North      44  950.860863         4  41837.877982\n",
      "\n",
      "[81 rows x 4 columns]\n",
      "\n",
      "All Laptop sales across all regions on 2023-01-01:\n",
      "        sales       price  quantity       revenue\n",
      "region                                           \n",
      "East       43  814.401850        16  35019.279556\n",
      "North      72  694.820345        12  50027.064809\n",
      "South      85  548.691221        17  46638.753790\n",
      "West       65  458.953513        10  29831.978337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.1.1 Advanced Indexing and MultiIndex\n",
    "print(\"🔹 Advanced Indexing and MultiIndex:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a larger dataset for advanced operations\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(\"2023-01-01\", periods=100, freq=\"D\")\n",
    "products = [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Headphones\"]\n",
    "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "\n",
    "# Create sales data with MultiIndex\n",
    "sales_data = []\n",
    "for date in dates:\n",
    "    for product in products:\n",
    "        for region in regions:\n",
    "            sales_data.append(\n",
    "                {\n",
    "                    \"date\": date,\n",
    "                    \"product\": product,\n",
    "                    \"region\": region,\n",
    "                    \"sales\": random.randint(10, 100),\n",
    "                    \"price\": random.uniform(50, 1000),\n",
    "                    \"quantity\": random.randint(1, 20),\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "df_sales[\"revenue\"] = df_sales[\"sales\"] * df_sales[\"price\"]\n",
    "\n",
    "# Set MultiIndex\n",
    "df_sales_multi = df_sales.set_index([\"date\", \"product\", \"region\"]).sort_index()\n",
    "print(\"Sales DataFrame with MultiIndex:\")\n",
    "print(df_sales_multi.head(10))\n",
    "print()\n",
    "\n",
    "# Advanced indexing with MultiIndex\n",
    "print(\"Sales for Laptop in North region:\")\n",
    "laptop_north = df_sales_multi.loc[(\"2023-01-01\", \"Laptop\", \"North\") : (\"2023-01-05\", \"Laptop\", \"North\")]\n",
    "print(laptop_north)\n",
    "print()\n",
    "\n",
    "# Cross-section operations\n",
    "print(\"All Laptop sales across all regions on 2023-01-01:\")\n",
    "laptop_sales = df_sales_multi.xs((\"2023-01-01\", \"Laptop\"), level=[\"date\", \"product\"])\n",
    "print(laptop_sales)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb298f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Pivot Tables and Advanced Reshaping:\n",
      "--------------------------------------------------\n",
      "Revenue Pivot Table (Date vs Product):\n",
      "product        Headphones       Keyboard         Laptop        Monitor  \\\n",
      "date                                                                     \n",
      "2023-01-01   96043.594664  153788.336777  161517.076492  171817.005216   \n",
      "2023-01-02  114614.916598  169931.358316  220309.619458  157786.949811   \n",
      "2023-01-03   82686.838747   31750.536241   70085.813250   72223.589261   \n",
      "2023-01-04   75905.779011   86718.741533  117350.031910   99126.809574   \n",
      "2023-01-05  143504.383075  110227.555443  124496.733037  140823.053892   \n",
      "2023-01-06  177527.472571  207104.312580  116632.609279  118226.087375   \n",
      "2023-01-07   94802.695172  154768.333324  103971.043304  100307.644881   \n",
      "2023-01-08  155068.909009  140019.574994  129901.355209   90760.274920   \n",
      "2023-01-09   97249.527610   87315.276409  115159.285305  139279.709158   \n",
      "2023-01-10  108835.509184  131746.487846  100767.151469  150157.167361   \n",
      "\n",
      "product             Mouse  \n",
      "date                       \n",
      "2023-01-01  133981.735256  \n",
      "2023-01-02  131947.238268  \n",
      "2023-01-03  129082.254313  \n",
      "2023-01-04  153968.604344  \n",
      "2023-01-05  175968.660555  \n",
      "2023-01-06   95962.438522  \n",
      "2023-01-07  101282.716653  \n",
      "2023-01-08  100939.047954  \n",
      "2023-01-09  139108.987602  \n",
      "2023-01-10   96688.430232  \n",
      "\n",
      "Multi-level Pivot Table:\n",
      "                    quantity                                     revenue  \\\n",
      "product           Headphones Keyboard Laptop Monitor Mouse    Headphones   \n",
      "date       region                                                          \n",
      "2023-01-01 East         19.0      5.0   16.0     2.0   3.0   6739.318414   \n",
      "           North        18.0      3.0   12.0    12.0  14.0  18068.424045   \n",
      "           South         2.0      5.0   17.0    19.0  10.0  63213.137666   \n",
      "           West         19.0      1.0   10.0    17.0   4.0   8022.714539   \n",
      "2023-01-02 East         18.0      2.0    7.0     3.0   3.0  33881.263681   \n",
      "\n",
      "                                                                           \n",
      "product                Keyboard        Laptop       Monitor         Mouse  \n",
      "date       region                                                          \n",
      "2023-01-01 East    23608.388261  35019.279556  15781.594551  49264.327688  \n",
      "           North   62083.361746  50027.064809  38275.378609  13708.442260  \n",
      "           South   22037.116228  46638.753790  38325.043642  62196.222143  \n",
      "           West    46059.470542  29831.978337  79434.988414   8812.743165  \n",
      "2023-01-02 East    62500.460296  12183.600258  71059.570937  27666.768193  \n",
      "\n",
      "Melted DataFrame (first 10 rows):\n",
      "        date product        revenue\n",
      "0 2023-01-01  Laptop  161517.076492\n",
      "1 2023-01-02  Laptop  220309.619458\n",
      "2 2023-01-03  Laptop   70085.813250\n",
      "3 2023-01-04  Laptop  117350.031910\n",
      "4 2023-01-05  Laptop  124496.733037\n",
      "5 2023-01-06  Laptop  116632.609279\n",
      "6 2023-01-07  Laptop  103971.043304\n",
      "7 2023-01-08  Laptop  129901.355209\n",
      "8 2023-01-09  Laptop  115159.285305\n",
      "9 2023-01-10  Laptop  100767.151469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.1.2 Pivot Tables and Advanced Reshaping\n",
    "print(\"🔹 Pivot Tables and Advanced Reshaping:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Reset index for pivot operations\n",
    "df_sales_pivot = df_sales.copy()\n",
    "\n",
    "# Create pivot table\n",
    "pivot_sales = df_sales_pivot.pivot_table(values=\"revenue\", index=\"date\", columns=\"product\", aggfunc=\"sum\", fill_value=0)\n",
    "print(\"Revenue Pivot Table (Date vs Product):\")\n",
    "print(pivot_sales.head(10))\n",
    "print()\n",
    "\n",
    "# Multi-level pivot table\n",
    "pivot_multi = df_sales_pivot.pivot_table(\n",
    "    values=[\"revenue\", \"quantity\"],\n",
    "    index=[\"date\", \"region\"],\n",
    "    columns=\"product\",\n",
    "    aggfunc={\"revenue\": \"sum\", \"quantity\": \"mean\"},\n",
    "    fill_value=0,\n",
    ")\n",
    "print(\"Multi-level Pivot Table:\")\n",
    "print(pivot_multi.head())\n",
    "print()\n",
    "\n",
    "# Melt operation (unpivot)\n",
    "melted_sales = pivot_sales.reset_index().melt(id_vars=\"date\", value_vars=products, var_name=\"product\", value_name=\"revenue\")\n",
    "print(\"Melted DataFrame (first 10 rows):\")\n",
    "print(melted_sales.head(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d36ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Window Functions and Rolling Operations:\n",
      "--------------------------------------------------\n",
      "Time Series with Rolling Statistics:\n",
      "Time Series with Rolling Statistics:\n",
      "         date  daily_revenue  revenue_7day_avg  revenue_30day_std\n",
      "0  2023-01-01  717147.748406     717147.748406                NaN\n",
      "5  2023-01-01  717147.748406     717147.748406                0.0\n",
      "7  2023-01-01  717147.748406     717147.748406                0.0\n",
      "6  2023-01-01  717147.748406     717147.748406                0.0\n",
      "1  2023-01-01  717147.748406     717147.748406                0.0\n",
      "4  2023-01-01  717147.748406     717147.748406                0.0\n",
      "2  2023-01-01  717147.748406     717147.748406                0.0\n",
      "9  2023-01-01  717147.748406     717147.748406                0.0\n",
      "8  2023-01-01  717147.748406     717147.748406                0.0\n",
      "14 2023-01-01  717147.748406     717147.748406                0.0\n",
      "3  2023-01-01  717147.748406     717147.748406                0.0\n",
      "11 2023-01-01  717147.748406     717147.748406                0.0\n",
      "10 2023-01-01  717147.748406     717147.748406                0.0\n",
      "18 2023-01-01  717147.748406     717147.748406                0.0\n",
      "17 2023-01-01  717147.748406     717147.748406                0.0\n",
      "\n",
      "Monthly Product Rankings (by revenue):\n",
      "    date  product  region  sales  price  quantity  revenue\n",
      "0  610.5    310.5   388.0  195.0  201.0     260.0    113.0\n",
      "1  610.5    310.5   233.0  116.5  295.0     107.5    131.0\n",
      "2  610.5    310.5   543.0  407.0  120.0     142.5    213.0\n",
      "3  610.5    310.5    78.0  243.5  368.0     323.5    256.0\n",
      "4  610.5     62.5   388.0  158.5  550.0     204.5    448.0\n",
      "5  610.5     62.5   233.0  141.0  149.0     323.5     61.0\n",
      "6  610.5     62.5   543.0  158.5  245.0     544.0    117.0\n",
      "7  610.5     62.5    78.0  618.0   69.0     512.5    509.0\n",
      "8  610.5    434.5   388.0  126.0  171.0     544.0     62.0\n",
      "9  610.5    434.5   233.0  310.0  410.0     480.5    338.0\n",
      "\n",
      "Cumulative Operations:\n",
      "Cumulative Operations:\n",
      "        date       revenue  revenue_cumsum  revenue_cummax\n",
      "0 2023-01-01  50027.064809    50027.064809    50027.064809\n",
      "1 2023-01-01  46638.753790    96665.818599    50027.064809\n",
      "2 2023-01-01  35019.279556   131685.098155    50027.064809\n",
      "3 2023-01-01  29831.978337   161517.076492    50027.064809\n",
      "4 2023-01-01  13708.442260   175225.518752    50027.064809\n",
      "5 2023-01-01  62196.222143   237421.740895    62196.222143\n",
      "6 2023-01-01  49264.327688   286686.068584    62196.222143\n",
      "7 2023-01-01   8812.743165   295498.811748    62196.222143\n",
      "8 2023-01-01  62083.361746   357582.173495    62196.222143\n",
      "9 2023-01-01  22037.116228   379619.289723    62196.222143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.1.3 Advanced Window Functions and Rolling Operations\n",
    "print(\"🔹 Window Functions and Rolling Operations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create time series data\n",
    "# Use the original df_sales for time series operations\n",
    "df_ts = df_sales.copy()\n",
    "df_ts = df_ts.sort_values(\"date\")\n",
    "df_ts = df_ts.sort_values(\"date\")\n",
    "\n",
    "# Rolling operations\n",
    "# Create a simple time series from the sales data\n",
    "df_ts[\"daily_revenue\"] = df_ts.groupby(\"date\")[\"revenue\"].transform(\"sum\")\n",
    "df_ts[\"revenue_7day_avg\"] = df_ts[\"daily_revenue\"].rolling(window=7, min_periods=1).mean()\n",
    "df_ts[\"revenue_30day_std\"] = df_ts[\"daily_revenue\"].rolling(window=30, min_periods=1).std()\n",
    "\n",
    "print(\"Time Series with Rolling Statistics:\")\n",
    "print(\"Time Series with Rolling Statistics:\")\n",
    "print(df_ts[[\"date\", \"daily_revenue\", \"revenue_7day_avg\", \"revenue_30day_std\"]].head(15))\n",
    "print()\n",
    "\n",
    "# Window functions with groupby\n",
    "# Use the original sales data for grouped operations\n",
    "df_sales_grouped = df_sales.copy()\n",
    "df_sales_grouped[\"date\"] = pd.to_datetime(df_sales_grouped[\"date\"])\n",
    "df_sales_grouped[\"month\"] = df_sales_grouped[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "# Calculate monthly rankings\n",
    "monthly_rankings = df_sales_grouped.groupby(\"month\").rank(ascending=False)\n",
    "print(\"Monthly Product Rankings (by revenue):\")\n",
    "print(monthly_rankings.head(10))\n",
    "print()\n",
    "\n",
    "# Cumulative operations\n",
    "df_sales_grouped[\"revenue_cumsum\"] = df_sales_grouped[\"revenue\"].cumsum()\n",
    "df_sales_grouped[\"revenue_cummax\"] = df_sales_grouped[\"revenue\"].cummax()\n",
    "df_sales_grouped[\"revenue_cumsum\"] = df_sales_grouped[\"revenue\"].cumsum()\n",
    "df_sales_grouped[\"revenue_cummax\"] = df_sales_grouped[\"revenue\"].cummax()\n",
    "print(\"Cumulative Operations:\")\n",
    "print(\"Cumulative Operations:\")\n",
    "print(df_sales_grouped[[\"date\", \"revenue\", \"revenue_cumsum\", \"revenue_cummax\"]].head(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561d2a5",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Performance Optimization and Memory Management\n",
    "\n",
    "Learn how to optimize pandas operations for better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfade975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Memory Optimization Techniques:\n",
      "--------------------------------------------------\n",
      "Original DataFrame Memory Usage:\n",
      "Memory usage: 1.67 MB\n",
      "Data types:\n",
      "id            int64\n",
      "category     object\n",
      "status       object\n",
      "region       object\n",
      "value       float64\n",
      "dtype: object\n",
      "\n",
      "Optimized DataFrame Memory Usage:\n",
      "Memory usage: 0.18 MB\n",
      "Data types:\n",
      "id             int64\n",
      "category    category\n",
      "status      category\n",
      "region      category\n",
      "value        float64\n",
      "dtype: object\n",
      "\n",
      "Memory savings: 89.1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2.1 Memory optimization with categorical data types\n",
    "print(\"🔹 Memory Optimization Techniques:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a large dataset to demonstrate memory optimization\n",
    "np.random.seed(42)\n",
    "n_records = 10000\n",
    "\n",
    "large_data = {\n",
    "    \"id\": range(n_records),\n",
    "    \"category\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\"], n_records),\n",
    "    \"status\": np.random.choice([\"Active\", \"Inactive\", \"Pending\"], n_records),\n",
    "    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n_records),\n",
    "    \"value\": np.random.uniform(0, 1000, n_records),\n",
    "}\n",
    "\n",
    "df_large = pd.DataFrame(large_data)\n",
    "print(\"Original DataFrame Memory Usage:\")\n",
    "print(f\"Memory usage: {df_large.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Data types:\\n{df_large.dtypes}\")\n",
    "print()\n",
    "\n",
    "# Convert to categorical for memory optimization\n",
    "df_optimized = df_large.copy()\n",
    "df_optimized[\"category\"] = df_optimized[\"category\"].astype(\"category\")\n",
    "df_optimized[\"status\"] = df_optimized[\"status\"].astype(\"category\")\n",
    "df_optimized[\"region\"] = df_optimized[\"region\"].astype(\"category\")\n",
    "\n",
    "print(\"Optimized DataFrame Memory Usage:\")\n",
    "print(f\"Memory usage: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Data types:\\n{df_optimized.dtypes}\")\n",
    "print()\n",
    "\n",
    "# Memory savings\n",
    "original_memory = df_large.memory_usage(deep=True).sum()\n",
    "optimized_memory = df_optimized.memory_usage(deep=True).sum()\n",
    "savings = (original_memory - optimized_memory) / original_memory * 100\n",
    "print(f\"Memory savings: {savings:.1f}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4584fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Performance Comparison: Vectorized vs Loops:\n",
      "--------------------------------------------------\n",
      "Loop method time: 1.6658 seconds\n",
      "Vectorized method time: 0.0004 seconds\n",
      "Speed improvement: 4176.4x faster\n",
      "\n",
      "Apply method time: 0.2749 seconds\n",
      "Vectorized vs Apply speed improvement: 689.3x faster\n",
      "\n",
      "Results verification (first 5 rows):\n",
      "   result_loop  result_vectorized  result_apply\n",
      "0     0.864434           0.864434      0.864434\n",
      "1     2.420560           2.420560      2.420560\n",
      "2    -1.872403          -1.872403     -1.872403\n",
      "3    -0.038637          -0.038637     -0.038637\n",
      "4     0.024619           0.024619      0.024619\n",
      "All results equal: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2.2 Vectorized operations vs loops\n",
    "print(\"🔹 Performance Comparison: Vectorized vs Loops:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Create test data\n",
    "n = 100000\n",
    "df_perf = pd.DataFrame({\"a\": np.random.randn(n), \"b\": np.random.randn(n), \"c\": np.random.randn(n)})\n",
    "\n",
    "# Method 1: Using loops (slow)\n",
    "start_time = time()\n",
    "result_loop = []\n",
    "for i in range(len(df_perf)):\n",
    "    result_loop.append(df_perf.iloc[i][\"a\"] * df_perf.iloc[i][\"b\"] + df_perf.iloc[i][\"c\"])\n",
    "df_perf[\"result_loop\"] = result_loop\n",
    "loop_time = time() - start_time\n",
    "\n",
    "# Method 2: Using vectorized operations (fast)\n",
    "start_time = time()\n",
    "df_perf[\"result_vectorized\"] = df_perf[\"a\"] * df_perf[\"b\"] + df_perf[\"c\"]\n",
    "vectorized_time = time() - start_time\n",
    "\n",
    "print(f\"Loop method time: {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized method time: {vectorized_time:.4f} seconds\")\n",
    "print(f\"Speed improvement: {loop_time / vectorized_time:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "# Method 3: Using apply (moderate speed)\n",
    "start_time = time()\n",
    "df_perf[\"result_apply\"] = df_perf.apply(lambda row: row[\"a\"] * row[\"b\"] + row[\"c\"], axis=1)\n",
    "apply_time = time() - start_time\n",
    "\n",
    "print(f\"Apply method time: {apply_time:.4f} seconds\")\n",
    "print(f\"Vectorized vs Apply speed improvement: {apply_time / vectorized_time:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "# Verify results are the same\n",
    "print(\"Results verification (first 5 rows):\")\n",
    "print(df_perf[[\"result_loop\", \"result_vectorized\", \"result_apply\"]].head())\n",
    "print(f\"All results equal: {np.allclose(df_perf['result_loop'], df_perf['result_vectorized'])}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f83eb",
   "metadata": {},
   "source": [
    "# 🌟 LEVEL 4: REAL-WORLD SCENARIOS AND PRACTICAL EXAMPLES\n",
    "\n",
    "## Exercise 4.1: E-commerce Analytics Dashboard\n",
    "\n",
    "Let's build a comprehensive analytics solution using all the techniques we've learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b08fbd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 E-commerce Analytics Dashboard:\n",
      "--------------------------------------------------\n",
      "Dataset Overview:\n",
      "Customers: 1000\n",
      "Products: 50\n",
      "Orders: 5000\n",
      "\n",
      "Sample Customer Data:\n",
      "  customer_id        name  age          city membership_tier  \\\n",
      "0   CUST_0001  Customer_1   56       Phoenix          Bronze   \n",
      "1   CUST_0002  Customer_2   69      San Jose            Gold   \n",
      "2   CUST_0003  Customer_3   46        Dallas          Bronze   \n",
      "3   CUST_0004  Customer_4   32  Philadelphia            Gold   \n",
      "4   CUST_0005  Customer_5   60      New York          Silver   \n",
      "\n",
      "              registration_date  \n",
      "0 2020-01-01 00:00:00.000000000  \n",
      "1 2020-01-02 11:04:30.270270270  \n",
      "2 2020-01-03 22:09:00.540540540  \n",
      "3 2020-01-05 09:13:30.810810810  \n",
      "4 2020-01-06 20:18:01.081081081  \n",
      "\n",
      "Sample Product Data:\n",
      "  product_id       name    category       price        cost    supplier\n",
      "0   PROD_001  Product_1  Automotive  346.429058  110.591379  Supplier_A\n",
      "1   PROD_002  Product_2      Sports   66.098943   83.122370  Supplier_C\n",
      "2   PROD_003  Product_3       Books  198.081053  147.638693  Supplier_C\n",
      "3   PROD_004  Product_4       Books  233.555889   95.937244  Supplier_B\n",
      "4   PROD_005  Product_5        Toys  190.835981  152.263171  Supplier_D\n",
      "\n",
      "Sample Order Data:\n",
      "      order_id customer_id                    order_date product_id  quantity  \\\n",
      "0  ORDER_00001   CUST_0139 2023-01-01 00:00:00.000000000   PROD_011         2   \n",
      "1  ORDER_00002   CUST_0996 2023-01-01 01:44:51.178235647   PROD_015         5   \n",
      "2  ORDER_00003   CUST_0435 2023-01-01 03:29:42.356471294   PROD_003         5   \n",
      "3  ORDER_00004   CUST_0509 2023-01-01 05:14:33.534706941   PROD_005         1   \n",
      "4  ORDER_00005   CUST_0273 2023-01-01 06:59:24.712942588   PROD_023         1   \n",
      "\n",
      "   discount_percent  shipping_cost  \n",
      "0          0.174886      21.101443  \n",
      "1          0.020856      10.640167  \n",
      "2          0.014305      19.456406  \n",
      "3          0.018337       7.924515  \n",
      "4          0.282563       9.745635  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1.1 Create comprehensive e-commerce dataset\n",
    "print(\"🔹 E-commerce Analytics Dashboard:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create comprehensive e-commerce data\n",
    "n_orders = 5000\n",
    "n_customers = 1000\n",
    "n_products = 50\n",
    "\n",
    "# Customer data\n",
    "customers = {\n",
    "    \"customer_id\": [f\"CUST_{i:04d}\" for i in range(1, n_customers + 1)],\n",
    "    \"name\": [f\"Customer_{i}\" for i in range(1, n_customers + 1)],\n",
    "    \"age\": np.random.randint(18, 80, n_customers),\n",
    "    \"city\": np.random.choice(\n",
    "        [\n",
    "            \"New York\",\n",
    "            \"Los Angeles\",\n",
    "            \"Chicago\",\n",
    "            \"Houston\",\n",
    "            \"Phoenix\",\n",
    "            \"Philadelphia\",\n",
    "            \"San Antonio\",\n",
    "            \"San Diego\",\n",
    "            \"Dallas\",\n",
    "            \"San Jose\",\n",
    "        ],\n",
    "        n_customers,\n",
    "    ),\n",
    "    \"membership_tier\": np.random.choice([\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"], n_customers, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    \"registration_date\": pd.date_range(\"2020-01-01\", \"2023-12-31\", periods=n_customers),\n",
    "}\n",
    "\n",
    "# Product data\n",
    "products = {\n",
    "    \"product_id\": [f\"PROD_{i:03d}\" for i in range(1, n_products + 1)],\n",
    "    \"name\": [f\"Product_{i}\" for i in range(1, n_products + 1)],\n",
    "    \"category\": np.random.choice(\n",
    "        [\n",
    "            \"Electronics\",\n",
    "            \"Clothing\",\n",
    "            \"Books\",\n",
    "            \"Home & Garden\",\n",
    "            \"Sports\",\n",
    "            \"Beauty\",\n",
    "            \"Toys\",\n",
    "            \"Automotive\",\n",
    "        ],\n",
    "        n_products,\n",
    "    ),\n",
    "    \"price\": np.random.uniform(10, 500, n_products),\n",
    "    \"cost\": np.random.uniform(5, 250, n_products),\n",
    "    \"supplier\": np.random.choice([\"Supplier_A\", \"Supplier_B\", \"Supplier_C\", \"Supplier_D\"], n_products),\n",
    "}\n",
    "\n",
    "# Order data\n",
    "orders = {\n",
    "    \"order_id\": [f\"ORDER_{i:05d}\" for i in range(1, n_orders + 1)],\n",
    "    \"customer_id\": np.random.choice(customers[\"customer_id\"], n_orders),\n",
    "    \"order_date\": pd.date_range(\"2023-01-01\", \"2023-12-31\", periods=n_orders),\n",
    "    \"product_id\": np.random.choice(products[\"product_id\"], n_orders),\n",
    "    \"quantity\": np.random.randint(1, 10, n_orders),\n",
    "    \"discount_percent\": np.random.uniform(0, 0.3, n_orders),\n",
    "    \"shipping_cost\": np.random.uniform(5, 25, n_orders),\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "df_customers = pd.DataFrame(customers)\n",
    "df_products = pd.DataFrame(products)\n",
    "df_orders = pd.DataFrame(orders)\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Customers: {len(df_customers)}\")\n",
    "print(f\"Products: {len(df_products)}\")\n",
    "print(f\"Orders: {len(df_orders)}\")\n",
    "print()\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample Customer Data:\")\n",
    "print(df_customers.head())\n",
    "print()\n",
    "\n",
    "print(\"Sample Product Data:\")\n",
    "print(df_products.head())\n",
    "print()\n",
    "\n",
    "print(\"Sample Order Data:\")\n",
    "print(df_orders.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "700f6862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Advanced Data Joins and Analytics:\n",
      "--------------------------------------------------\n",
      "Complete Dataset with Derived Metrics:\n",
      "      order_id customer_id product_id  order_value  final_order_value  \\\n",
      "0  ORDER_00001   CUST_0139   PROD_011   367.675866         303.374547   \n",
      "1  ORDER_00002   CUST_0996   PROD_015   936.703978         917.167647   \n",
      "2  ORDER_00003   CUST_0435   PROD_003   990.405263         976.237572   \n",
      "3  ORDER_00004   CUST_0509   PROD_005   190.835981         187.336648   \n",
      "4  ORDER_00005   CUST_0273   PROD_023    69.073624          49.556007   \n",
      "\n",
      "       profit order_month  \n",
      "0  287.804735     2023-01  \n",
      "1  880.530472     2023-01  \n",
      "2  252.211799     2023-01  \n",
      "3   38.572810     2023-01  \n",
      "4  -84.342292     2023-01  \n",
      "\n",
      "📊 KEY PERFORMANCE INDICATORS:\n",
      "----------------------------------------\n",
      "Total Revenue: $5,333,840.50\n",
      "Total Orders: 5,000\n",
      "Average Order Value: $1066.77\n",
      "Total Profit: $3,356,987.28\n",
      "Profit Margin: 62.9%\n",
      "\n",
      "Unique Customers: 989\n",
      "Average Orders per Customer: 5.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1.2 Advanced data joins and comprehensive analytics\n",
    "print(\"🔹 Advanced Data Joins and Analytics:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Join all datasets\n",
    "df_complete = df_orders.merge(df_customers, on=\"customer_id\", how=\"left\")\n",
    "df_complete = df_complete.merge(df_products, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Calculate derived metrics\n",
    "df_complete[\"order_value\"] = df_complete[\"quantity\"] * df_complete[\"price\"]\n",
    "df_complete[\"discount_amount\"] = df_complete[\"order_value\"] * df_complete[\"discount_percent\"]\n",
    "df_complete[\"final_order_value\"] = df_complete[\"order_value\"] - df_complete[\"discount_amount\"]\n",
    "df_complete[\"profit\"] = df_complete[\"quantity\"] * (df_complete[\"price\"] - df_complete[\"cost\"])\n",
    "df_complete[\"total_cost\"] = df_complete[\"final_order_value\"] + df_complete[\"shipping_cost\"]\n",
    "\n",
    "# Add time-based features\n",
    "df_complete[\"order_month\"] = df_complete[\"order_date\"].dt.to_period(\"M\")\n",
    "df_complete[\"order_quarter\"] = df_complete[\"order_date\"].dt.to_period(\"Q\")\n",
    "df_complete[\"order_weekday\"] = df_complete[\"order_date\"].dt.day_name()\n",
    "df_complete[\"order_hour\"] = np.random.randint(0, 24, len(df_complete))  # Simulated hour\n",
    "\n",
    "print(\"Complete Dataset with Derived Metrics:\")\n",
    "print(\n",
    "    df_complete[\n",
    "        [\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            \"product_id\",\n",
    "            \"order_value\",\n",
    "            \"final_order_value\",\n",
    "            \"profit\",\n",
    "            \"order_month\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "print()\n",
    "\n",
    "# Key Performance Indicators (KPIs)\n",
    "print(\"📊 KEY PERFORMANCE INDICATORS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Revenue metrics\n",
    "total_revenue = df_complete[\"final_order_value\"].sum()\n",
    "total_orders = len(df_complete)\n",
    "avg_order_value = df_complete[\"final_order_value\"].mean()\n",
    "total_profit = df_complete[\"profit\"].sum()\n",
    "\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"Total Orders: {total_orders:,}\")\n",
    "print(f\"Average Order Value: ${avg_order_value:.2f}\")\n",
    "print(f\"Total Profit: ${total_profit:,.2f}\")\n",
    "print(f\"Profit Margin: {(total_profit / total_revenue) * 100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Customer metrics\n",
    "unique_customers = df_complete[\"customer_id\"].nunique()\n",
    "avg_orders_per_customer = total_orders / unique_customers\n",
    "\n",
    "print(f\"Unique Customers: {unique_customers:,}\")\n",
    "print(f\"Average Orders per Customer: {avg_orders_per_customer:.1f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4666545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Advanced Analytics and Insights:\n",
      "--------------------------------------------------\n",
      "🏆 TOP 10 PRODUCTS BY SALES:\n",
      "            total_sales  total_quantity  order_count\n",
      "product_id                                          \n",
      "PROD_012      224825.48             562          107\n",
      "PROD_039      214674.49             592          113\n",
      "PROD_024      201326.01             533          104\n",
      "PROD_050      190731.01             485           99\n",
      "PROD_019      186236.72             479          105\n",
      "PROD_048      185739.00             520          101\n",
      "PROD_049      185424.62             506          103\n",
      "PROD_021      183411.20             503           95\n",
      "PROD_037      171821.55             543          100\n",
      "PROD_042      168174.20             583          115\n",
      "\n",
      "👥 CUSTOMER SEGMENTATION:\n",
      "              customer_count  avg_spent  avg_orders  avg_quantity\n",
      "segment                                                          \n",
      "High Value               198    6788.79        5.96         30.56\n",
      "Low Value                396    2482.00        3.35         14.84\n",
      "Medium Value             197    4931.40        4.80         23.78\n",
      "VIP                      198   10279.31        7.82         42.53\n",
      "\n",
      "📈 MONTHLY TRENDS:\n",
      "             monthly_sales  order_count  total_quantity\n",
      "order_month                                            \n",
      "2023-07          481432.35          426            2242\n",
      "2023-08          437420.98          426            2060\n",
      "2023-09          430413.62          412            1996\n",
      "2023-10          444637.65          425            2099\n",
      "2023-11          431469.65          412            2012\n",
      "2023-12          422969.08          413            2083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1.3 Advanced analytics and insights\n",
    "print(\"🔹 Advanced Analytics and Insights:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Top performing products\n",
    "top_products = (\n",
    "    df_complete.groupby(\"product_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"final_order_value\": \"sum\",  # Use final_order_value instead\n",
    "            \"quantity\": \"sum\",\n",
    "            \"order_id\": \"count\",\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "top_products.columns = [\"total_sales\", \"total_quantity\", \"order_count\"]\n",
    "top_products = top_products.sort_values(\"total_sales\", ascending=False).head(10)\n",
    "\n",
    "print(\"🏆 TOP 10 PRODUCTS BY SALES:\")\n",
    "print(top_products)\n",
    "print()\n",
    "\n",
    "# Customer segmentation analysis - FIXED: using 'city' instead of 'region'\n",
    "customer_analysis = (\n",
    "    df_complete.groupby(\"customer_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"final_order_value\": [\"sum\", \"mean\", \"count\"],  # Changed to final_order_value\n",
    "            \"quantity\": \"sum\",\n",
    "            \"city\": \"first\",  # Changed from 'region' to 'city'\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "customer_analysis.columns = [\n",
    "    \"total_spent\",\n",
    "    \"avg_order_value\",\n",
    "    \"order_count\",\n",
    "    \"total_quantity\",\n",
    "    \"city\",  # Changed column name\n",
    "]\n",
    "\n",
    "customer_analysis = customer_analysis.sort_values(\"total_spent\", ascending=False)\n",
    "\n",
    "\n",
    "# Customer segments\n",
    "def categorize_customer(row):\n",
    "    if row[\"total_spent\"] > customer_analysis[\"total_spent\"].quantile(0.8):\n",
    "        return \"VIP\"\n",
    "    elif row[\"total_spent\"] > customer_analysis[\"total_spent\"].quantile(0.6):\n",
    "        return \"High Value\"\n",
    "    elif row[\"total_spent\"] > customer_analysis[\"total_spent\"].quantile(0.4):\n",
    "        return \"Medium Value\"\n",
    "    else:\n",
    "        return \"Low Value\"\n",
    "\n",
    "\n",
    "customer_analysis[\"segment\"] = customer_analysis.apply(categorize_customer, axis=1)\n",
    "\n",
    "print(\"👥 CUSTOMER SEGMENTATION:\")\n",
    "segment_summary = (\n",
    "    customer_analysis.groupby(\"segment\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"total_spent\": [\"count\", \"mean\"],\n",
    "            \"order_count\": \"mean\",\n",
    "            \"total_quantity\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "segment_summary.columns = [\"customer_count\", \"avg_spent\", \"avg_orders\", \"avg_quantity\"]\n",
    "print(segment_summary)\n",
    "print()\n",
    "\n",
    "# Monthly trends\n",
    "monthly_trends = (\n",
    "    df_complete.groupby(\"order_month\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"final_order_value\": \"sum\",  # Use final_order_value\n",
    "            \"order_id\": \"count\",\n",
    "            \"quantity\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "monthly_trends.columns = [\"monthly_sales\", \"order_count\", \"total_quantity\"]\n",
    "print(\"📈 MONTHLY TRENDS:\")\n",
    "print(monthly_trends.tail(6))  # Last 6 months\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb6d4f",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Data Quality and Validation\n",
    "\n",
    "Learn how to handle data quality issues and validate your datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fa96e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Data Quality Assessment and Cleaning:\n",
      "--------------------------------------------------\n",
      "Dataset with Quality Issues:\n",
      "   id        name             email  age         salary        phone  \\\n",
      "0   1  Customer_1   user1@email.com   56   52574.539167  +1-555-0001   \n",
      "1   2  Customer_2   user2@email.com   69   85643.808593  +1-555-0002   \n",
      "2   3  Customer_3   user3@email.com   46   72402.267363  +1-555-0003   \n",
      "3   4  Customer_4   user4@email.com   32  100038.733422  +1-555-0004   \n",
      "4   5  Customer_5   user5@email.com   60   39328.156436  +1-555-0005   \n",
      "5   6  Customer_6   user6@email.com   25  146927.376920  +1-555-0006   \n",
      "6   7  Customer_7   user7@email.com   78  148345.289338  +1-555-0007   \n",
      "7   8  Customer_8   user8@email.com   38  113779.405682  +1-555-0008   \n",
      "8   9  Customer_9   user9@email.com   56   94331.563961  +1-555-0009   \n",
      "9  10        None  user10@email.com   75   67143.313954  +1-555-0010   \n",
      "\n",
      "       city registration_date  \n",
      "0  New York        2023-10-30  \n",
      "1   Houston        2023-10-21  \n",
      "2   Chicago        2023-05-23  \n",
      "3  New York        2023-07-10  \n",
      "4   Houston        2023-01-21  \n",
      "5  New York        2023-08-15  \n",
      "6  New York        2023-11-09  \n",
      "7   Chicago               NaT  \n",
      "8   Phoenix        2023-04-27  \n",
      "9   Phoenix        2023-10-29  \n",
      "\n",
      "📊 DATA QUALITY ASSESSMENT:\n",
      "----------------------------------------\n",
      "Missing Values Analysis:\n",
      "                   Missing_Count  Missing_Percentage\n",
      "name                         100                10.0\n",
      "salary                        40                 4.0\n",
      "registration_date            125                12.5\n",
      "\n",
      "Data Types:\n",
      "id                            int64\n",
      "name                         object\n",
      "email                        object\n",
      "age                           int64\n",
      "salary                      float64\n",
      "phone                        object\n",
      "city                         object\n",
      "registration_date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Outlier Analysis (using IQR method):\n",
      "id: 0 outliers\n",
      "age: 0 outliers\n",
      "salary: 0 outliers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.2.1 Data quality assessment and cleaning\n",
    "print(\"🔹 Data Quality Assessment and Cleaning:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a dataset with intentional quality issues\n",
    "np.random.seed(42)\n",
    "n_records = 1000\n",
    "\n",
    "# Create data with various quality issues\n",
    "quality_data = {\n",
    "    \"id\": range(1, n_records + 1),\n",
    "    \"name\": [f\"Customer_{i}\" if i % 10 != 0 else None for i in range(1, n_records + 1)],  # Missing names\n",
    "    \"email\": [f\"user{i}@email.com\" if i % 15 != 0 else \"invalid_email\" for i in range(1, n_records + 1)],  # Invalid emails\n",
    "    \"age\": [np.random.randint(18, 80) if i % 20 != 0 else -5 for i in range(1, n_records + 1)],  # Invalid ages\n",
    "    \"salary\": [np.random.uniform(30000, 150000) if i % 25 != 0 else None for i in range(1, n_records + 1)],  # Missing salaries\n",
    "    \"phone\": [f\"+1-555-{i:04d}\" if i % 30 != 0 else \"invalid_phone\" for i in range(1, n_records + 1)],  # Invalid phones\n",
    "    \"city\": [\n",
    "        (np.random.choice([\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"]) if i % 12 != 0 else \"Unknown_City\")\n",
    "        for i in range(1, n_records + 1)\n",
    "    ],  # Unknown cities\n",
    "    \"registration_date\": [\n",
    "        (pd.Timestamp(\"2023-01-01\") + pd.Timedelta(days=np.random.randint(0, 365)) if i % 8 != 0 else pd.NaT)\n",
    "        for i in range(1, n_records + 1)\n",
    "    ],  # Missing dates\n",
    "}\n",
    "\n",
    "df_quality = pd.DataFrame(quality_data)\n",
    "print(\"Dataset with Quality Issues:\")\n",
    "print(df_quality.head(10))\n",
    "print()\n",
    "\n",
    "# Data quality assessment\n",
    "print(\"📊 DATA QUALITY ASSESSMENT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_data = df_quality.isnull().sum()\n",
    "missing_percent = (missing_data / len(df_quality)) * 100\n",
    "\n",
    "quality_report = pd.DataFrame({\"Missing_Count\": missing_data, \"Missing_Percentage\": missing_percent.round(2)})\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(quality_report[quality_report[\"Missing_Count\"] > 0])\n",
    "print()\n",
    "\n",
    "# Data type analysis\n",
    "print(\"Data Types:\")\n",
    "print(df_quality.dtypes)\n",
    "print()\n",
    "\n",
    "# Duplicate analysis\n",
    "duplicates = df_quality.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "print()\n",
    "\n",
    "# Outlier detection for numeric columns\n",
    "numeric_cols = df_quality.select_dtypes(include=[np.number]).columns\n",
    "print(\"Outlier Analysis (using IQR method):\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_quality.columns:\n",
    "        Q1 = df_quality[col].quantile(0.25)\n",
    "        Q3 = df_quality[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df_quality[(df_quality[col] < lower_bound) | (df_quality[col] > upper_bound)]\n",
    "        print(f\"{col}: {len(outliers)} outliers\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b63895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Data Cleaning and Validation:\n",
      "--------------------------------------------------\n",
      "Available columns in df_complete:\n",
      "['order_id', 'customer_id', 'order_date', 'product_id', 'quantity', 'discount_percent', 'shipping_cost', 'name_x', 'age', 'city', 'membership_tier', 'registration_date', 'name_y', 'category', 'price', 'cost', 'supplier', 'order_value', 'discount_amount', 'final_order_value', 'profit', 'total_cost', 'order_month', 'order_quarter', 'order_weekday', 'order_hour']\n",
      "\n",
      "1. Handling Missing Values:\n",
      "Before cleaning - Missing values:\n",
      "Series([], dtype: int64)\n",
      "Total missing values: 0\n",
      "\n",
      "After cleaning - Missing values: 0\n",
      "\n",
      "2. Numeric Data Validation:\n",
      "Invalid ages found: 0\n",
      "Invalid prices found: 0\n",
      "Invalid quantities found: 0\n",
      "\n",
      "3. Categorical Data Validation:\n",
      "Invalid membership tiers found: 0\n",
      "Invalid cities found: 0\n",
      "Invalid categories found: 0\n",
      "\n",
      "4. Duplicate Records:\n",
      "Duplicate records found: 0\n",
      "\n",
      "📊 FINAL QUALITY CHECK:\n",
      "--------------------------------------------------\n",
      "Total records: 5000\n",
      "Missing values: 0\n",
      "Duplicate records: 0\n",
      "\n",
      "Data types summary:\n",
      "object            10\n",
      "float64            9\n",
      "int64              3\n",
      "datetime64[ns]     2\n",
      "period[M]          1\n",
      "period[Q-DEC]      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric columns summary:\n",
      "       quantity  discount_percent  shipping_cost      age    price     cost  \\\n",
      "count   5000.00           5000.00        5000.00  5000.00  5000.00  5000.00   \n",
      "mean       5.01              0.15          15.01    49.36   250.86   116.75   \n",
      "std        2.55              0.09           5.79    18.28   138.98    70.40   \n",
      "min        1.00              0.00           5.00    18.00    14.96    10.72   \n",
      "25%        3.00              0.07           9.98    33.00   154.57    58.38   \n",
      "50%        5.00              0.15          14.93    50.00   248.14   105.36   \n",
      "75%        7.00              0.22          20.07    66.00   364.75   170.89   \n",
      "max        9.00              0.30          24.99    79.00   471.45   247.07   \n",
      "\n",
      "       order_value  discount_amount  final_order_value   profit  total_cost  \\\n",
      "count      5000.00          5000.00            5000.00  5000.00     5000.00   \n",
      "mean       1254.23           187.47            1066.77   671.40     1081.78   \n",
      "std        1010.99           205.20             869.23   931.74      869.20   \n",
      "min          14.96             0.01              10.59 -1887.77       18.59   \n",
      "25%         394.31            38.33             323.59    38.57      340.63   \n",
      "50%         992.56           111.06             850.01   527.78      865.04   \n",
      "75%        1890.39           265.66            1624.04  1259.22     1641.92   \n",
      "max        4243.01          1233.94            4198.22  3914.47     4207.55   \n",
      "\n",
      "       order_hour  \n",
      "count     5000.00  \n",
      "mean        11.64  \n",
      "std          6.86  \n",
      "min          0.00  \n",
      "25%          6.00  \n",
      "50%         12.00  \n",
      "75%         18.00  \n",
      "max         23.00  \n",
      "\n",
      "Sample of cleaned data:\n",
      "      order_id customer_id                    order_date product_id  quantity  \\\n",
      "0  ORDER_00001   CUST_0139 2023-01-01 00:00:00.000000000   PROD_011         2   \n",
      "1  ORDER_00002   CUST_0996 2023-01-01 01:44:51.178235647   PROD_015         5   \n",
      "2  ORDER_00003   CUST_0435 2023-01-01 03:29:42.356471294   PROD_003         5   \n",
      "3  ORDER_00004   CUST_0509 2023-01-01 05:14:33.534706941   PROD_005         1   \n",
      "4  ORDER_00005   CUST_0273 2023-01-01 06:59:24.712942588   PROD_023         1   \n",
      "5  ORDER_00006   CUST_0099 2023-01-01 08:44:15.891178235   PROD_037         9   \n",
      "6  ORDER_00007   CUST_0764 2023-01-01 10:29:07.069413882   PROD_009         2   \n",
      "7  ORDER_00008   CUST_0850 2023-01-01 12:13:58.247649529   PROD_015         7   \n",
      "8  ORDER_00009   CUST_0104 2023-01-01 13:58:49.425885177   PROD_003         4   \n",
      "9  ORDER_00010   CUST_0214 2023-01-01 15:43:40.604120824   PROD_029         7   \n",
      "\n",
      "   discount_percent  shipping_cost        name_x  age         city  ...  \\\n",
      "0          0.174886      21.101443  Customer_139   25     San Jose  ...   \n",
      "1          0.020856      10.640167  Customer_996   18  Los Angeles  ...   \n",
      "2          0.014305      19.456406  Customer_435   63  San Antonio  ...   \n",
      "3          0.018337       7.924515  Customer_509   57      Chicago  ...   \n",
      "4          0.282563       9.745635  Customer_273   29      Phoenix  ...   \n",
      "5          0.254961      16.455530   Customer_99   61      Chicago  ...   \n",
      "6          0.253482      24.772717  Customer_764   72      Chicago  ...   \n",
      "7          0.277265       6.468889  Customer_850   36      Chicago  ...   \n",
      "8          0.151577      13.011092  Customer_104   31      Chicago  ...   \n",
      "9          0.074203       5.861619  Customer_214   68      Chicago  ...   \n",
      "\n",
      "     supplier  order_value discount_amount final_order_value       profit  \\\n",
      "0  Supplier_D   367.675866       64.301319        303.374547   287.804735   \n",
      "1  Supplier_A   936.703978       19.536331        917.167647   880.530472   \n",
      "2  Supplier_C   990.405263       14.167691        976.237572   252.211799   \n",
      "3  Supplier_D   190.835981        3.499333        187.336648    38.572810   \n",
      "4  Supplier_C    69.073624       19.517617         49.556007   -84.342292   \n",
      "5  Supplier_D  3305.655080      842.814542       2462.840539  2405.144274   \n",
      "6  Supplier_D    89.587511       22.708823         66.878689    -2.173988   \n",
      "7  Supplier_A  1311.385569      363.601357        947.784211  1232.742660   \n",
      "8  Supplier_C   792.324211      120.097823        672.226387   201.769439   \n",
      "9  Supplier_C  1517.577512      112.609413       1404.968099  1241.529173   \n",
      "\n",
      "    total_cost order_month  order_quarter  order_weekday  order_hour  \n",
      "0   324.475990     2023-01         2023Q1         Sunday          22  \n",
      "1   927.807813     2023-01         2023Q1         Sunday           0  \n",
      "2   995.693978     2023-01         2023Q1         Sunday           9  \n",
      "3   195.261162     2023-01         2023Q1         Sunday          17  \n",
      "4    59.301642     2023-01         2023Q1         Sunday          21  \n",
      "5  2479.296069     2023-01         2023Q1         Sunday           7  \n",
      "6    91.651405     2023-01         2023Q1         Sunday          19  \n",
      "7   954.253101     2023-01         2023Q1         Sunday           2  \n",
      "8   685.237479     2023-01         2023Q1         Sunday          18  \n",
      "9  1410.829718     2023-01         2023Q1         Sunday          18  \n",
      "\n",
      "[10 rows x 26 columns]\n",
      "\n",
      "Columns processed:\n",
      "- Total columns: 26\n",
      "- Numeric columns: 12\n",
      "- Categorical columns: 10\n",
      "- DateTime columns: 2\n"
     ]
    }
   ],
   "source": [
    "# 4.2.2 Data cleaning and validation\n",
    "print(\"🔹 Data Cleaning and Validation:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# First, let's see what columns we actually have\n",
    "print(\"Available columns in df_complete:\")\n",
    "print(df_complete.columns.tolist())\n",
    "print()\n",
    "\n",
    "# Create a cleaned version of the dataset\n",
    "df_cleaned = df_complete.copy()\n",
    "\n",
    "# 1. Handle missing values\n",
    "print(\"1. Handling Missing Values:\")\n",
    "print(f\"Before cleaning - Missing values:\\n{df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0]}\")\n",
    "print(f\"Total missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print()\n",
    "\n",
    "# Fill missing values based on actual columns\n",
    "if \"name_x\" in df_cleaned.columns:  # Customer name from merge\n",
    "    df_cleaned[\"name_x\"] = df_cleaned[\"name_x\"].fillna(\"Unknown Customer\")\n",
    "\n",
    "if \"name_y\" in df_cleaned.columns:  # Product name from merge\n",
    "    df_cleaned[\"name_y\"] = df_cleaned[\"name_y\"].fillna(\"Unknown Product\")\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        median_value = df_cleaned[col].median()\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(median_value)\n",
    "\n",
    "# Fill missing dates with a default date\n",
    "date_columns = df_cleaned.select_dtypes(include=[\"datetime64\"]).columns\n",
    "for col in date_columns:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(pd.Timestamp(\"2023-01-01\"))\n",
    "\n",
    "print(f\"After cleaning - Missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print()\n",
    "\n",
    "# 2. Validate numeric ranges\n",
    "print(\"2. Numeric Data Validation:\")\n",
    "\n",
    "# Validate age (if it exists)\n",
    "if \"age\" in df_cleaned.columns:\n",
    "    invalid_ages = df_cleaned[(df_cleaned[\"age\"] < 0) | (df_cleaned[\"age\"] > 120)]\n",
    "    print(f\"Invalid ages found: {len(invalid_ages)}\")\n",
    "\n",
    "    if len(invalid_ages) > 0:\n",
    "        median_age = df_cleaned[(df_cleaned[\"age\"] >= 0) & (df_cleaned[\"age\"] <= 120)][\"age\"].median()\n",
    "        df_cleaned.loc[(df_cleaned[\"age\"] < 0) | (df_cleaned[\"age\"] > 120), \"age\"] = median_age\n",
    "        print(f\"After cleaning - Invalid ages: {len(df_cleaned[(df_cleaned['age'] < 0) | (df_cleaned['age'] > 120)])}\")\n",
    "\n",
    "# Validate prices (should be positive)\n",
    "if \"price\" in df_cleaned.columns:\n",
    "    invalid_prices = df_cleaned[df_cleaned[\"price\"] < 0]\n",
    "    print(f\"Invalid prices found: {len(invalid_prices)}\")\n",
    "\n",
    "    if len(invalid_prices) > 0:\n",
    "        df_cleaned.loc[df_cleaned[\"price\"] < 0, \"price\"] = df_cleaned[df_cleaned[\"price\"] >= 0][\"price\"].median()\n",
    "\n",
    "# Validate quantities (should be positive)\n",
    "if \"quantity\" in df_cleaned.columns:\n",
    "    invalid_quantities = df_cleaned[df_cleaned[\"quantity\"] <= 0]\n",
    "    print(f\"Invalid quantities found: {len(invalid_quantities)}\")\n",
    "\n",
    "    if len(invalid_quantities) > 0:\n",
    "        df_cleaned.loc[df_cleaned[\"quantity\"] <= 0, \"quantity\"] = 1\n",
    "\n",
    "print()\n",
    "\n",
    "# 3. Validate categorical data\n",
    "print(\"3. Categorical Data Validation:\")\n",
    "\n",
    "# Validate membership_tier\n",
    "if \"membership_tier\" in df_cleaned.columns:\n",
    "    valid_tiers = [\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"]\n",
    "    invalid_tiers = df_cleaned[~df_cleaned[\"membership_tier\"].isin(valid_tiers)]\n",
    "    print(f\"Invalid membership tiers found: {len(invalid_tiers)}\")\n",
    "\n",
    "    if len(invalid_tiers) > 0:\n",
    "        df_cleaned.loc[~df_cleaned[\"membership_tier\"].isin(valid_tiers), \"membership_tier\"] = \"Bronze\"\n",
    "\n",
    "# Validate city data\n",
    "if \"city\" in df_cleaned.columns:\n",
    "    valid_cities = [\n",
    "        \"New York\",\n",
    "        \"Los Angeles\",\n",
    "        \"Chicago\",\n",
    "        \"Houston\",\n",
    "        \"Phoenix\",\n",
    "        \"Philadelphia\",\n",
    "        \"San Antonio\",\n",
    "        \"San Diego\",\n",
    "        \"Dallas\",\n",
    "        \"San Jose\",\n",
    "    ]\n",
    "    invalid_cities = df_cleaned[~df_cleaned[\"city\"].isin(valid_cities)]\n",
    "    print(f\"Invalid cities found: {len(invalid_cities)}\")\n",
    "\n",
    "    if len(invalid_cities) > 0:\n",
    "        df_cleaned.loc[~df_cleaned[\"city\"].isin(valid_cities), \"city\"] = \"Other\"\n",
    "\n",
    "# Validate category data\n",
    "if \"category\" in df_cleaned.columns:\n",
    "    valid_categories = [\n",
    "        \"Electronics\",\n",
    "        \"Clothing\",\n",
    "        \"Books\",\n",
    "        \"Home & Garden\",\n",
    "        \"Sports\",\n",
    "        \"Beauty\",\n",
    "        \"Toys\",\n",
    "        \"Automotive\",\n",
    "    ]\n",
    "    invalid_categories = df_cleaned[~df_cleaned[\"category\"].isin(valid_categories)]\n",
    "    print(f\"Invalid categories found: {len(invalid_categories)}\")\n",
    "\n",
    "    if len(invalid_categories) > 0:\n",
    "        df_cleaned.loc[~df_cleaned[\"category\"].isin(valid_categories), \"category\"] = \"Other\"\n",
    "\n",
    "print()\n",
    "\n",
    "# 4. Remove duplicates\n",
    "print(\"4. Duplicate Records:\")\n",
    "duplicates_before = df_cleaned.duplicated().sum()\n",
    "print(f\"Duplicate records found: {duplicates_before}\")\n",
    "\n",
    "if duplicates_before > 0:\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"After cleaning - Duplicate records: {df_cleaned.duplicated().sum()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Final quality check\n",
    "print(\"📊 FINAL QUALITY CHECK:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total records: {len(df_cleaned)}\")\n",
    "print(f\"Missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate records: {df_cleaned.duplicated().sum()}\")\n",
    "print()\n",
    "\n",
    "print(\"Data types summary:\")\n",
    "print(df_cleaned.dtypes.value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Numeric columns summary:\")\n",
    "print(df_cleaned.select_dtypes(include=[np.number]).describe().round(2))\n",
    "print()\n",
    "\n",
    "print(\"Sample of cleaned data:\")\n",
    "print(df_cleaned.head(10))\n",
    "print()\n",
    "\n",
    "# Show which columns had issues\n",
    "print(\"Columns processed:\")\n",
    "print(f\"- Total columns: {len(df_cleaned.columns)}\")\n",
    "print(f\"- Numeric columns: {len(df_cleaned.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"- Categorical columns: {len(df_cleaned.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"- DateTime columns: {len(df_cleaned.select_dtypes(include=['datetime64']).columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add62761",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Challenge Problems\n",
    "\n",
    "Test your pandas skills with these challenging exercises!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d51d788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 CHALLENGE 1: Complex Data Transformation\n",
      "--------------------------------------------------\n",
      "Complex Dataset Structure:\n",
      "  transaction_id customer_id  \\\n",
      "0      TXN_00001    CUST_064   \n",
      "1      TXN_00002    CUST_059   \n",
      "\n",
      "                                 transaction_details           timestamp  \n",
      "0  {'products': [{'product_id': 'PROD_011', 'quan... 2023-01-01 00:00:00  \n",
      "1  {'products': [{'product_id': 'PROD_014', 'quan... 2023-01-01 01:00:00  \n",
      "\n",
      "SOLUTION: Flattening nested data...\n",
      "Flattened Dataset:\n",
      "  transaction_id customer_id           timestamp product_id  quantity  \\\n",
      "0      TXN_00001    CUST_064 2023-01-01 00:00:00   PROD_011         2   \n",
      "1      TXN_00001    CUST_064 2023-01-01 00:00:00   PROD_005         3   \n",
      "2      TXN_00001    CUST_064 2023-01-01 00:00:00   PROD_012         2   \n",
      "3      TXN_00002    CUST_059 2023-01-01 01:00:00   PROD_014         3   \n",
      "4      TXN_00002    CUST_059 2023-01-01 01:00:00   PROD_018         3   \n",
      "5      TXN_00003    CUST_090 2023-01-01 02:00:00   PROD_009         3   \n",
      "6      TXN_00003    CUST_090 2023-01-01 02:00:00   PROD_009         2   \n",
      "7      TXN_00003    CUST_090 2023-01-01 02:00:00   PROD_001         4   \n",
      "8      TXN_00004    CUST_062 2023-01-01 03:00:00   PROD_015         1   \n",
      "9      TXN_00004    CUST_062 2023-01-01 03:00:00   PROD_014         2   \n",
      "\n",
      "        price payment_method            discount_codes  \n",
      "0   74.962003    Credit Card              [DISCOUNT_4]  \n",
      "1   99.164193    Credit Card              [DISCOUNT_4]  \n",
      "2  133.406697    Credit Card              [DISCOUNT_4]  \n",
      "3  190.348132     Debit Card  [DISCOUNT_5, DISCOUNT_5]  \n",
      "4  138.731007     Debit Card  [DISCOUNT_5, DISCOUNT_5]  \n",
      "5  142.455803           Cash              [DISCOUNT_6]  \n",
      "6  115.390665           Cash              [DISCOUNT_6]  \n",
      "7   88.627218           Cash              [DISCOUNT_6]  \n",
      "8  169.099745           Cash                        []  \n",
      "9   13.637525           Cash                        []  \n",
      "Original records: 1000\n",
      "Flattened records: 2472\n",
      "\n",
      "Transaction Totals:\n",
      "                total_quantity  total_value customer_id           timestamp  \\\n",
      "transaction_id                                                                \n",
      "TXN_00001                    7       714.23    CUST_064 2023-01-01 00:00:00   \n",
      "TXN_00002                    6       987.24    CUST_059 2023-01-01 01:00:00   \n",
      "TXN_00003                    9      1012.66    CUST_090 2023-01-01 02:00:00   \n",
      "TXN_00004                    3       196.37    CUST_062 2023-01-01 03:00:00   \n",
      "TXN_00005                   12      1385.23    CUST_076 2023-01-01 04:00:00   \n",
      "\n",
      "               payment_method  \n",
      "transaction_id                 \n",
      "TXN_00001         Credit Card  \n",
      "TXN_00002          Debit Card  \n",
      "TXN_00003                Cash  \n",
      "TXN_00004                Cash  \n",
      "TXN_00005         Credit Card  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q4/c7m5t5_d5ydbc4ts6qsrwf200000gn/T/ipykernel_12984/4115545145.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  \"timestamp\": pd.date_range(\"2023-01-01\", periods=1000, freq=\"H\"),\n"
     ]
    }
   ],
   "source": [
    "# 4.3.1 Challenge 1: Complex Data Transformation\n",
    "print(\"🔹 CHALLENGE 1: Complex Data Transformation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a complex dataset with nested structures\n",
    "challenge_data = {\n",
    "    \"transaction_id\": [f\"TXN_{i:05d}\" for i in range(1, 1001)],\n",
    "    \"customer_id\": [f\"CUST_{np.random.randint(1, 101):03d}\" for _ in range(1000)],\n",
    "    \"transaction_details\": [\n",
    "        {\n",
    "            \"products\": [\n",
    "                {\n",
    "                    \"product_id\": f\"PROD_{np.random.randint(1, 21):03d}\",\n",
    "                    \"quantity\": np.random.randint(1, 6),\n",
    "                    \"price\": np.random.uniform(10, 200),\n",
    "                }\n",
    "                for _ in range(np.random.randint(1, 5))\n",
    "            ],\n",
    "            \"payment_method\": np.random.choice([\"Credit Card\", \"Debit Card\", \"PayPal\", \"Cash\"]),\n",
    "            \"discount_codes\": [f\"DISCOUNT_{i}\" for i in np.random.choice(range(1, 11), np.random.randint(0, 3))],\n",
    "        }\n",
    "        for _ in range(1000)\n",
    "    ],\n",
    "    \"timestamp\": pd.date_range(\"2023-01-01\", periods=1000, freq=\"H\"),\n",
    "}\n",
    "\n",
    "df_challenge = pd.DataFrame(challenge_data)\n",
    "print(\"Complex Dataset Structure:\")\n",
    "print(df_challenge.head(2))\n",
    "print()\n",
    "\n",
    "# Challenge: Flatten the nested transaction_details\n",
    "print(\"SOLUTION: Flattening nested data...\")\n",
    "\n",
    "# Extract and flatten the nested data\n",
    "flattened_data = []\n",
    "for idx, row in df_challenge.iterrows():\n",
    "    transaction_id = row[\"transaction_id\"]\n",
    "    customer_id = row[\"customer_id\"]\n",
    "    timestamp = row[\"timestamp\"]\n",
    "\n",
    "    for product in row[\"transaction_details\"][\"products\"]:\n",
    "        flattened_data.append(\n",
    "            {\n",
    "                \"transaction_id\": transaction_id,\n",
    "                \"customer_id\": customer_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"product_id\": product[\"product_id\"],\n",
    "                \"quantity\": product[\"quantity\"],\n",
    "                \"price\": product[\"price\"],\n",
    "                \"payment_method\": row[\"transaction_details\"][\"payment_method\"],\n",
    "                \"discount_codes\": row[\"transaction_details\"][\"discount_codes\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_flattened = pd.DataFrame(flattened_data)\n",
    "print(\"Flattened Dataset:\")\n",
    "print(df_flattened.head(10))\n",
    "print(f\"Original records: {len(df_challenge)}\")\n",
    "print(f\"Flattened records: {len(df_flattened)}\")\n",
    "print()\n",
    "\n",
    "# Calculate total value per transaction\n",
    "transaction_totals = (\n",
    "    df_flattened.groupby(\"transaction_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"quantity\": \"sum\",\n",
    "            \"price\": lambda x: (x * df_flattened.loc[x.index, \"quantity\"]).sum(),\n",
    "            \"customer_id\": \"first\",\n",
    "            \"timestamp\": \"first\",\n",
    "            \"payment_method\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "transaction_totals.columns = [\n",
    "    \"total_quantity\",\n",
    "    \"total_value\",\n",
    "    \"customer_id\",\n",
    "    \"timestamp\",\n",
    "    \"payment_method\",\n",
    "]\n",
    "print(\"Transaction Totals:\")\n",
    "print(transaction_totals.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "409ee68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 CHALLENGE 2: Advanced Time Series Analysis\n",
      "--------------------------------------------------\n",
      "Time Series Dataset:\n",
      "                 sales  website_traffic  customer_satisfaction  \\\n",
      "date                                                             \n",
      "2020-01-01  104.967142       938.785088               3.961061   \n",
      "2020-01-02   99.478025      1145.915134               4.091118   \n",
      "2020-01-03  108.197966      1152.459560               4.221719   \n",
      "2020-01-04  117.811282      1057.750575               4.344341   \n",
      "2020-01-05  101.098588       942.652172               4.348327   \n",
      "2020-01-06  101.956870       888.509643               4.475955   \n",
      "2020-01-07  120.947213       863.367280               4.791152   \n",
      "2020-01-08  113.684750       940.205847               4.694351   \n",
      "2020-01-09  102.169195      1178.596430               4.671092   \n",
      "2020-01-10  113.141041      1254.817157               4.384420   \n",
      "\n",
      "            inventory_level  marketing_spend  \n",
      "date                                          \n",
      "2020-01-01       503.958333      1072.099695  \n",
      "2020-01-02       487.750554       980.359612  \n",
      "2020-01-03       491.712760      1362.313560  \n",
      "2020-01-04       485.974417      1004.520339  \n",
      "2020-01-05       470.946574      1045.673576  \n",
      "2020-01-06       502.416729      1114.470775  \n",
      "2020-01-07       511.105989      1227.681787  \n",
      "2020-01-08       509.597672      1187.715448  \n",
      "2020-01-09       515.598505      1326.192854  \n",
      "2020-01-10       505.586804      1196.033787  \n",
      "\n",
      "SOLUTION: Advanced Time Series Analysis...\n",
      "Time Series Analysis Results:\n",
      "Optimal lag between marketing spend and sales: 22 days\n",
      "Maximum correlation: 0.025\n",
      "\n",
      "Anomalies detected in sales: 0\n",
      "\n",
      "Performance Metrics:\n",
      "Sales growth rate (yearly): -1.55%\n",
      "Average daily sales: 100.45\n",
      "Sales volatility (CV): 36.43%\n",
      "\n",
      "Sample of enhanced time series data:\n",
      "                 sales  sales_7day_avg  sales_trend  sales_seasonal\n",
      "date                                                               \n",
      "2020-01-01  104.967142             NaN          NaN       -1.901799\n",
      "2020-01-02   99.478025             NaN          NaN       -4.046306\n",
      "2020-01-03  108.197966             NaN          NaN       -0.218516\n",
      "2020-01-04  117.811282             NaN          NaN        4.950383\n",
      "2020-01-05  101.098588             NaN          NaN        8.776315\n",
      "2020-01-06  101.956870             NaN          NaN       -2.507795\n",
      "2020-01-07  120.947213      107.779584          NaN       20.555655\n",
      "2020-01-08  113.684750      109.024956          NaN       14.627093\n",
      "2020-01-09  102.169195      109.409409          NaN       21.471784\n",
      "2020-01-10  113.141041      110.115563          NaN       -4.094561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.3.2 Challenge 2: Advanced Time Series Analysis\n",
    "print(\"🔹 CHALLENGE 2: Advanced Time Series Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create time series data with multiple patterns\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(\"2020-01-01\", \"2023-12-31\", freq=\"D\")\n",
    "n_days = len(dates)\n",
    "\n",
    "# Create multiple time series with different patterns\n",
    "time_series_data = {\n",
    "    \"date\": dates,\n",
    "    \"sales\": 100 + 50 * np.sin(np.arange(n_days) * 2 * np.pi / 365) + np.random.normal(0, 10, n_days),\n",
    "    \"website_traffic\": 1000 + 200 * np.sin(np.arange(n_days) * 2 * np.pi / 7) + np.random.normal(0, 50, n_days),\n",
    "    \"customer_satisfaction\": 4.0 + 0.5 * np.sin(np.arange(n_days) * 2 * np.pi / 30) + np.random.normal(0, 0.2, n_days),\n",
    "    \"inventory_level\": 500 - np.arange(n_days) * 0.1 + np.random.normal(0, 20, n_days),\n",
    "    \"marketing_spend\": 1000 + 500 * np.sin(np.arange(n_days) * 2 * np.pi / 90) + np.random.normal(0, 100, n_days),\n",
    "}\n",
    "\n",
    "df_timeseries = pd.DataFrame(time_series_data)\n",
    "df_timeseries.set_index(\"date\", inplace=True)\n",
    "\n",
    "print(\"Time Series Dataset:\")\n",
    "print(df_timeseries.head(10))\n",
    "print()\n",
    "\n",
    "# Challenge: Advanced time series analysis\n",
    "print(\"SOLUTION: Advanced Time Series Analysis...\")\n",
    "\n",
    "# 1. Rolling statistics\n",
    "df_timeseries[\"sales_7day_avg\"] = df_timeseries[\"sales\"].rolling(window=7).mean()\n",
    "df_timeseries[\"sales_30day_std\"] = df_timeseries[\"sales\"].rolling(window=30).std()\n",
    "df_timeseries[\"traffic_14day_trend\"] = (\n",
    "    df_timeseries[\"website_traffic\"].rolling(window=14).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])\n",
    ")\n",
    "\n",
    "# 2. Seasonal decomposition (simplified)\n",
    "\n",
    "\n",
    "def seasonal_decomposition(series, period=365):\n",
    "    # Simple moving average for trend\n",
    "    trend = series.rolling(window=period, center=True).mean()\n",
    "\n",
    "    # Detrended series\n",
    "    detrended = series - trend\n",
    "\n",
    "    # Seasonal component (average of detrended values for each day of year)\n",
    "    seasonal = detrended.groupby(detrended.index.dayofyear).mean()\n",
    "\n",
    "    # Map seasonal component back to original index\n",
    "    seasonal_component = detrended.index.map(lambda x: seasonal[x.dayofyear])\n",
    "\n",
    "    # Residual\n",
    "    residual = detrended - seasonal_component\n",
    "\n",
    "    return trend, seasonal_component, residual\n",
    "\n",
    "\n",
    "trend, seasonal, residual = seasonal_decomposition(df_timeseries[\"sales\"])\n",
    "df_timeseries[\"sales_trend\"] = trend\n",
    "df_timeseries[\"sales_seasonal\"] = seasonal\n",
    "df_timeseries[\"sales_residual\"] = residual\n",
    "\n",
    "# 3. Correlation analysis with lag\n",
    "\n",
    "\n",
    "def cross_correlation(series1, series2, max_lag=30):\n",
    "    correlations = []\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "\n",
    "    for lag in lags:\n",
    "        if lag == 0:\n",
    "            corr = series1.corr(series2)\n",
    "        elif lag > 0:\n",
    "            corr = series1.shift(lag).corr(series2)\n",
    "        else:\n",
    "            corr = series1.corr(series2.shift(-lag))\n",
    "        correlations.append(corr)\n",
    "\n",
    "    return pd.Series(correlations, index=lags)\n",
    "\n",
    "\n",
    "# Find optimal lag between marketing spend and sales\n",
    "marketing_sales_corr = cross_correlation(df_timeseries[\"marketing_spend\"], df_timeseries[\"sales\"])\n",
    "optimal_lag = marketing_sales_corr.idxmax()\n",
    "\n",
    "print(\"Time Series Analysis Results:\")\n",
    "print(f\"Optimal lag between marketing spend and sales: {optimal_lag} days\")\n",
    "print(f\"Maximum correlation: {marketing_sales_corr.max():.3f}\")\n",
    "print()\n",
    "\n",
    "# 4. Anomaly detection using Z-score\n",
    "\n",
    "\n",
    "def detect_anomalies(series, threshold=3):\n",
    "    z_scores = np.abs((series - series.mean()) / series.std())\n",
    "    return z_scores > threshold\n",
    "\n",
    "\n",
    "anomalies = detect_anomalies(df_timeseries[\"sales\"])\n",
    "print(f\"Anomalies detected in sales: {anomalies.sum()}\")\n",
    "print()\n",
    "\n",
    "# 5. Performance metrics\n",
    "print(\"Performance Metrics:\")\n",
    "print(\n",
    "    f\"Sales growth rate (yearly): {((df_timeseries['sales'].iloc[-1] / df_timeseries['sales'].iloc[0]) ** (365 / len(df_timeseries)) - 1) * 100:.2f}%\"\n",
    ")\n",
    "print(f\"Average daily sales: {df_timeseries['sales'].mean():.2f}\")\n",
    "print(f\"Sales volatility (CV): {(df_timeseries['sales'].std() / df_timeseries['sales'].mean()) * 100:.2f}%\")\n",
    "print()\n",
    "\n",
    "print(\"Sample of enhanced time series data:\")\n",
    "print(df_timeseries[[\"sales\", \"sales_7day_avg\", \"sales_trend\", \"sales_seasonal\"]].head(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd2aeb",
   "metadata": {},
   "source": [
    "# 🎉 CONGRATULATIONS!\n",
    "\n",
    "You've completed a comprehensive journey through pandas from basic to advanced level! \n",
    "\n",
    "## What You've Learned:\n",
    "\n",
    "### 📚 **Level 1: Basic Operations**\n",
    "- Creating DataFrames from different data types (lists, dictionaries, tuples, sets)\n",
    "- Basic DataFrame operations and inspection\n",
    "- Column manipulation and data selection\n",
    "\n",
    "### 🎯 **Level 2: Intermediate Operations**\n",
    "- Advanced filtering and conditional operations\n",
    "- Grouping and aggregation techniques\n",
    "- Working with complex data types (lists, dictionaries, sets, tuples in columns)\n",
    "- Multi-level grouping and custom aggregations\n",
    "\n",
    "### 🚀 **Level 3: Advanced Operations**\n",
    "- MultiIndex and advanced indexing\n",
    "- Pivot tables and data reshaping\n",
    "- Window functions and rolling operations\n",
    "- Performance optimization and memory management\n",
    "- Vectorized operations vs loops\n",
    "\n",
    "### 🌟 **Level 4: Real-World Applications**\n",
    "- E-commerce analytics dashboard\n",
    "- Data quality assessment and cleaning\n",
    "- Complex data transformations\n",
    "- Advanced time series analysis\n",
    "- Challenge problems with nested data structures\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "1. **Data Types Mastery**: You now understand how to work with all Python data types within pandas\n",
    "2. **Performance Optimization**: You know how to write efficient pandas code\n",
    "3. **Real-World Skills**: You can handle complex, messy datasets like those in production\n",
    "4. **Advanced Analytics**: You can perform sophisticated data analysis and transformations\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "- Practice with your own datasets\n",
    "- Explore pandas documentation for additional features\n",
    "- Learn about pandas integration with other libraries (matplotlib, seaborn, scikit-learn)\n",
    "- Try working with larger datasets to practice performance optimization\n",
    "\n",
    "**Happy coding with pandas! 🐼✨**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
